{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# State Shaper\n",
    "\n",
    "A state shaper is used to convert an environment observation to a state vector as input to value or policy models by extracting relevant temporal and spatial information. The scenario-specific call method returns the the ID of the agent involved in the current decision event and the shaped state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maro.rl import StateShaper\n",
    "\n",
    "\n",
    "class CIMStateShaper(StateShaper):\n",
    "    def __init__(self, *, look_back, max_ports_downstream, port_attributes, vessel_attributes):\n",
    "        super().__init__()\n",
    "        self._look_back = look_back\n",
    "        self._max_ports_downstream = max_ports_downstream\n",
    "        self._port_attributes = port_attributes\n",
    "        self._vessel_attributes = vessel_attributes\n",
    "        self._dim = (look_back + 1) * (max_ports_downstream + 1) * len(port_attributes) + len(vessel_attributes)\n",
    "\n",
    "    def __call__(self, decision_event, snapshot_list):\n",
    "        tick, port_idx, vessel_idx = decision_event.tick, decision_event.port_idx, decision_event.vessel_idx\n",
    "        ticks = [tick - rt for rt in range(self._look_back-1)]\n",
    "        future_port_idx_list = snapshot_list[\"vessels\"][tick: vessel_idx: 'future_stop_list'].astype('int')\n",
    "        port_features = snapshot_list[\"ports\"][ticks: [port_idx] + list(future_port_idx_list): self._port_attributes]\n",
    "        vessel_features = snapshot_list[\"vessels\"][tick: vessel_idx: self._vessel_attributes]\n",
    "        state = np.concatenate((port_features, vessel_features))\n",
    "        return str(port_idx), state\n",
    "    \n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Shaper\n",
    "\n",
    "An action shaper is used to convert an agent's output to an Action object which can be executed by the env's step() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.rl import ActionShaper\n",
    "from maro.simulator.scenarios.cim.common import Action\n",
    "\n",
    "\n",
    "class CIMActionShaper(ActionShaper):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__()\n",
    "        self._action_space = action_space\n",
    "        self._zero_action_index = action_space.index(0)\n",
    "\n",
    "    def __call__(self, model_action, decision_event, snapshot_list):\n",
    "        assert 0 <= model_action < len(self._action_space)\n",
    "        \n",
    "        scope = decision_event.action_scope\n",
    "        tick = decision_event.tick\n",
    "        port_idx = decision_event.port_idx\n",
    "        vessel_idx = decision_event.vessel_idx\n",
    "        port_empty = snapshot_list[\"ports\"][tick: port_idx: [\"empty\", \"full\", \"on_shipper\", \"on_consignee\"]][0]\n",
    "        vessel_remaining_space = snapshot_list[\"vessels\"][tick: vessel_idx: [\"empty\", \"full\", \"remaining_space\"]][2]\n",
    "        early_discharge = snapshot_list[\"vessels\"][tick:vessel_idx: \"early_discharge\"][0]\n",
    "\n",
    "        if model_action < self._zero_action_index:\n",
    "            actual_action = max(round(self._action_space[model_action] * port_empty), -vessel_remaining_space)\n",
    "        elif model_action > self._zero_action_index:\n",
    "            plan_action = self._action_space[model_action] * (scope.discharge + early_discharge) - early_discharge\n",
    "            actual_action = round(plan_action) if plan_action > 0 else round(self._action_space[model_action] * scope.discharge)\n",
    "        else:\n",
    "            actual_action = 0\n",
    "\n",
    "        return Action(vessel_idx, port_idx, actual_action)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Experience Shaper\n",
    "\n",
    "An experience shaper is responsible for converting the trajectory from an episode to experiences that can be used by the agents' training algorithms. It is necessary to specify how to determine the reward for an action given the business metrics associated with the corresponding transition. Here we showcase a custom reward shaper for the contained inventory management (CIM) scenario.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "from maro.rl import ExperienceShaper\n",
    "\n",
    "\n",
    "class TruncatedExperienceShaper(ExperienceShaper):\n",
    "    def __init__(self, *, time_window: int, time_decay_factor: float, fulfillment_factor: float,\n",
    "                 shortage_factor: float):\n",
    "        super().__init__(reward_func=None)\n",
    "        self._time_window = time_window\n",
    "        self._time_decay_factor = time_decay_factor\n",
    "        self._fulfillment_factor = fulfillment_factor\n",
    "        self._shortage_factor = shortage_factor\n",
    "\n",
    "    def __call__(self, trajectory, snapshot_list):\n",
    "        experiences_by_agent = {}\n",
    "        for i in range(len(trajectory) - 1):\n",
    "            transition = trajectory[i]\n",
    "            agent_id = transition[\"agent_id\"]\n",
    "            if agent_id not in experiences_by_agent:\n",
    "                experiences_by_agent[agent_id] = defaultdict(list)\n",
    "            \n",
    "            experiences = experiences_by_agent[agent_id]\n",
    "            experiences[\"state\"].append(transition[\"state\"])\n",
    "            experiences[\"action\"].append(transition[\"action\"])\n",
    "            experiences[\"reward\"].append(self._compute_reward(transition[\"event\"], snapshot_list))\n",
    "            experiences[\"next_state\"].append(trajectory[i+1][\"state\"])\n",
    "\n",
    "        return experiences_by_agent\n",
    "\n",
    "    def _compute_reward(self, decision_event, snapshot_list):\n",
    "        start_tick = decision_event.tick + 1\n",
    "        end_tick = decision_event.tick + self._time_window\n",
    "        ticks = list(range(start_tick, end_tick))\n",
    "\n",
    "        # calculate tc reward\n",
    "        future_fulfillment = snapshot_list[\"ports\"][ticks::\"fulfillment\"]\n",
    "        future_shortage = snapshot_list[\"ports\"][ticks::\"shortage\"]\n",
    "        decay_list = [self._time_decay_factor ** i for i in range(end_tick - start_tick)\n",
    "                      for _ in range(future_fulfillment.shape[0]//(end_tick-start_tick))]\n",
    "\n",
    "        tot_fulfillment = np.dot(future_fulfillment, decay_list)\n",
    "        tot_shortage = np.dot(future_shortage, decay_list)\n",
    "\n",
    "        return np.float(self._fulfillment_factor * tot_fulfillment - self._shortage_factor * tot_shortage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "An agent is a combination of (RL) algorithm, experience pool, and a set of non-algorithm-specific parameters (algorithm-specific parameters are managed by the algorithm module). Non-algorithm-specific parameters are used to manage experience storage, sampling strategies, and training strategies. Since all kinds of scenario-specific stuff will be handled by the agent manager, the agent is scenario agnostic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.rl import AbsAgent, ColumnBasedStore\n",
    "\n",
    "\n",
    "class CIMAgent(AbsAgent):\n",
    "    def __init__(self, name, algorithm, experience_pool: ColumnBasedStore, min_experiences_to_train,\n",
    "                 num_batches, batch_size):\n",
    "        super().__init__(name, algorithm, experience_pool)\n",
    "        self._min_experiences_to_train = min_experiences_to_train\n",
    "        self._num_batches = num_batches\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "    def train(self):\n",
    "        if len(self._experience_pool) < self._min_experiences_to_train:\n",
    "            return\n",
    "\n",
    "        for _ in range(self._num_batches):\n",
    "            indexes, sample = self._experience_pool.sample_by_key(\"loss\", self._batch_size)\n",
    "            state = np.asarray(sample[\"state\"])\n",
    "            action = np.asarray(sample[\"action\"])\n",
    "            reward = np.asarray(sample[\"reward\"])\n",
    "            next_state = np.asarray(sample[\"next_state\"])\n",
    "            loss = self._algorithm.train(state, action, reward, next_state)\n",
    "            self._experience_pool.update(indexes, {\"loss\": loss})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Manager\n",
    "\n",
    "An agent manager manages all agents and provides a unified interface with the environment. The agent manager is responsible for both inference and training. It is composed of a state shaper, an action shaper and an experience shaper which perform necessary conversions so that the underlying agents do not need to concern themselves with the business logic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import yaml\n",
    "\n",
    "from torch.nn.functional import smooth_l1_loss\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "from maro.rl import AbsAgentManager, LearningModel, MLPDecisionLayers, DQN, DQNHyperParams, ColumnBasedStore\n",
    "\n",
    "\n",
    "num_actions = 21\n",
    "\n",
    "\n",
    "class DQNAgentManager(AbsAgentManager):\n",
    "    def _assemble(self, agent_dict):\n",
    "        for agent_id in self._agent_id_list:\n",
    "            eval_model = LearningModel(decision_layers=MLPDecisionLayers(name=f'{agent_id}.policy',\n",
    "                                                                         input_dim=self._state_shaper.dim,\n",
    "                                                                         output_dim=num_actions,\n",
    "                                                                         hidden_dims=[256, 128, 64],\n",
    "                                                                         dropout_p=.0)\n",
    "                                       )\n",
    "\n",
    "            algorithm = DQN(model_dict={\"eval\": eval_model},\n",
    "                            optimizer_opt=(RMSprop, {\"lr\": 0.05}),\n",
    "                            loss_func_dict={\"eval\": smooth_l1_loss},\n",
    "                            hyper_params=DQNHyperParams(num_actions=num_actions, reward_decay=.0,\n",
    "                                                        num_training_rounds_per_target_replacement=5, tau=0.1)\n",
    "                           )\n",
    "\n",
    "            experience_pool = ColumnBasedStore()\n",
    "            \n",
    "            agent_dict[agent_id] = CIMAgent(name=agent_id, algorithm=algorithm, experience_pool=experience_pool,\n",
    "                                            min_experiences_to_train=1024, num_batches=10, batch_size=128)\n",
    "\n",
    "    def store_experiences(self, experiences):\n",
    "        for agent_id, exp in experiences.items():\n",
    "            exp.update({\"loss\": [1e8] * len(exp[next(iter(exp))])})\n",
    "            self._agent_dict[agent_id].store_experiences(exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop\n",
    "\n",
    "The code below demonstrates the typical structure of a program using MARO. One starts by creating an environment. Next, shapers and an explorer are created and an agent manager is created that loads these components. The creation of the agent manager also assembles all agents under the hood. Because the code is for the single-host mode, the agent manager mode is set to TRAIN_INFERENCE. Next, an actor is created to wrap the env and agent manager, and a learner is created to wrap the same agent manager and the actor. Finally, the task is started by calling the learner's train() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.simulator import Env\n",
    "from maro.rl import SimpleLearner, SimpleActor, AgentMode, TwoPhaseLinearExplorer\n",
    "from maro.utils import Logger\n",
    "\n",
    "\n",
    "env = Env(\"cim\", \"toy.4p_ssdd_l0.0\", durations=1120)\n",
    "total_episodes = 100\n",
    "agent_id_list = [str(agent_id) for agent_id in env.agent_idx_list]\n",
    "\n",
    "state_shaper = CIMStateShaper(look_back=7, max_ports_downstream=2, \n",
    "                              port_attributes=[\"empty\", \"full\", \"on_shipper\", \"on_consignee\", \n",
    "                                               \"booking\", \"shortage\", \"fulfillment\"],\n",
    "                              vessel_attributes=[\"empty\", \"full\", \"remaining_space\"]\n",
    "                             )\n",
    "\n",
    "action_shaper = CIMActionShaper(action_space=list(np.linspace(-1.0, 1.0, num_actions)))\n",
    "\n",
    "experience_shaper = TruncatedExperienceShaper(time_window=100, fulfillment_factor=1.0, shortage_factor=1.0,\n",
    "                                              time_decay_factor=0.97)\n",
    "\n",
    "explorer = TwoPhaseLinearExplorer(agent_id_list, total_episodes, \n",
    "                                  epsilon_range_dict={\"_all_\": (.0, .4)},\n",
    "                                  split_point_dict={\"_all_\": (.5, .8)},\n",
    "                                  with_cache=True)\n",
    "\n",
    "agent_manager = DQNAgentManager(name=\"cim_learner\",\n",
    "                                mode=AgentMode.TRAIN_INFERENCE,\n",
    "                                agent_id_list=agent_id_list,\n",
    "                                state_shaper=state_shaper,\n",
    "                                action_shaper=action_shaper,\n",
    "                                experience_shaper=experience_shaper,\n",
    "                                explorer=explorer)\n",
    "\n",
    "learner = SimpleLearner(trainable_agents=agent_manager, actor=SimpleActor(env, agent_manager),\n",
    "                        logger=Logger(\"single_host_cim_learner\", auto_timestamp=False))\n",
    "\n",
    "learner.train(total_episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
