{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# state shaper\n",
    "\n",
    "A state shaper is used to convert an environment observation to a state vector as input to value or policy models by extracting relevant temporal and spatial information. The scenario-specific __call__ method returns the the ID of the agent involved in the current decision event and the shaped state. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maro.rl import AbstractStateShaper\n",
    "\n",
    "\n",
    "class ECRStateShaper(AbstractStateShaper):\n",
    "    def __init__(self, *, look_back, max_ports_downstream, port_attributes, vessel_attributes):\n",
    "        super().__init__()\n",
    "        self._look_back = look_back\n",
    "        self._max_ports_downstream = max_ports_downstream\n",
    "        self._port_attributes = port_attributes\n",
    "        self._vessel_attributes = vessel_attributes\n",
    "        self._dim = (look_back + 1) * (max_ports_downstream + 1) * len(port_attributes) + len(vessel_attributes)\n",
    "\n",
    "    def __call__(self, decision_event, snapshot_list):\n",
    "        tick, port_idx, vessel_idx = decision_event.tick, decision_event.port_idx, decision_event.vessel_idx\n",
    "        ticks = [tick - rt for rt in range(self._look_back-1)]\n",
    "        future_port_idx_list = snapshot_list[\"vessels\"][tick: vessel_idx: 'future_stop_list'].astype('int')\n",
    "        port_features = snapshot_list[\"ports\"][ticks: [port_idx] + list(future_port_idx_list): self._port_attributes]\n",
    "        vessel_features = snapshot_list[\"vessels\"][tick: vessel_idx: self._vessel_attributes]\n",
    "        state = np.concatenate((port_features, vessel_features))\n",
    "        return str(port_idx), state\n",
    "    \n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# action shaper\n",
    "\n",
    "An action shaper is used to convert the output of an underlying algorithm's choose_action() method to an Action object which can be executed by the env's step() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.rl import AbstractActionShaper\n",
    "from maro.simulator.scenarios.ecr.common import Action\n",
    "\n",
    "\n",
    "class ECRActionShaper(AbstractActionShaper):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__()\n",
    "        self._action_space = action_space\n",
    "        self._zero_action_index = action_space.index(0)\n",
    "\n",
    "    def __call__(self, model_action, decision_event, snapshot_list):\n",
    "        scope = decision_event.action_scope\n",
    "        tick = decision_event.tick\n",
    "        port_idx = decision_event.port_idx\n",
    "        vessel_idx = decision_event.vessel_idx\n",
    "\n",
    "        port_empty = snapshot_list[\"ports\"][tick: port_idx: [\"empty\", \"full\", \"on_shipper\", \"on_consignee\"]][0]\n",
    "        vessel_remaining_space = snapshot_list[\"vessels\"][tick: vessel_idx: [\"empty\", \"full\", \"remaining_space\"]][2]\n",
    "        early_discharge = snapshot_list[\"vessels\"][tick:vessel_idx: \"early_discharge\"][0]\n",
    "        assert 0 <= model_action < len(self._action_space)\n",
    "\n",
    "        if model_action < self._zero_action_index:\n",
    "            actual_action = max(round(self._action_space[model_action] * port_empty), -vessel_remaining_space)\n",
    "        elif model_action > self._zero_action_index:\n",
    "            plan_action = self._action_space[model_action] * (scope.discharge + early_discharge) - early_discharge\n",
    "            actual_action = round(plan_action) if plan_action > 0 else round(self._action_space[model_action] * scope.discharge)\n",
    "        else:\n",
    "            actual_action = 0\n",
    "\n",
    "        return Action(vessel_idx, port_idx, actual_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reward shaper\n",
    "\n",
    "A reward shaper is used to record transitions during a roll-out episode and perform necessary post-processing at the end of the episode. The post-processing logic is encapsulated in the abstract shape() method and needs to be implemented for each scenario. It is necessary to compute rewards and next-states (and also next-actions for SARSA-like on-policy algorithms) during post-processing as they are set to None during the episode. In particular, it is necessary to specify how to determine the reward for an action given the business metrics associated with the corresponding transition. MARO provides the KStepRewardShaper class which may be combined with a user-defined reward function to form a default reward shaper. Here we showcase a custom reward shaper for the ECR scenario.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from maro.rl import AbstractRewardShaper, ExperienceKey, ExperienceInfoKey\n",
    "\n",
    "\n",
    "class ECRRewardShaper(AbstractRewardShaper):\n",
    "    def __init__(self, *, agent_id_list, time_window: int, time_decay_factor: float,\n",
    "                 fulfillment_factor: float, shortage_factor: float):\n",
    "        super().__init__()\n",
    "        self._agent_id_list = agent_id_list\n",
    "        self._time_window = time_window\n",
    "        self._time_decay_factor = time_decay_factor\n",
    "        self._fulfillment_factor = fulfillment_factor\n",
    "        self._shortage_factor = shortage_factor\n",
    "\n",
    "    def _shape(self, snapshot_list):\n",
    "        for i in range(len(self._trajectory[ExperienceKey.STATE])-1):\n",
    "            metrics = self._trajectory[ExperienceKey.INFO][i][ExperienceInfoKey.METRICS]\n",
    "            event = pickle.loads(self._trajectory[ExperienceKey.INFO][i][ExperienceInfoKey.EVENT])\n",
    "            self._trajectory[ExperienceKey.REWARD][i] = self._compute_reward(metrics, event, snapshot_list)\n",
    "            self._trajectory[ExperienceKey.NEXT_STATE][i] = self._trajectory[ExperienceKey.STATE][i+1]\n",
    "            self._trajectory[ExperienceKey.NEXT_ACTION][i] = self._trajectory[ExperienceKey.ACTION][i+1]\n",
    "            self._trajectory[ExperienceKey.INFO][i][ExperienceInfoKey.DISCOUNT] = .0\n",
    "\n",
    "    def _compute_reward(self, metrics, decision_event, snapshot_list):\n",
    "        start_tick = decision_event.tick + 1\n",
    "        end_tick = decision_event.tick + self._time_window\n",
    "        ticks = list(range(start_tick, end_tick))\n",
    "\n",
    "        # calculate tc reward\n",
    "        decay_list = [self._time_decay_factor ** i for i in range(end_tick - start_tick)\n",
    "                      for _ in range(len(self._agent_id_list))]\n",
    "\n",
    "        tot_fulfillment = np.dot(snapshot_list[\"ports\"][ticks::\"fulfillment\"], decay_list)\n",
    "        tot_shortage = np.dot(snapshot_list[\"ports\"][ticks::\"shortage\"], decay_list)\n",
    "\n",
    "        return np.float(self._fulfillment_factor * tot_fulfillment - self._shortage_factor * tot_shortage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agent manager\n",
    "\n",
    "An agent manager manages all agents and provides a unified interface with the environment. It is composed of a state shaper and an action shaper which perform necessary conversions so that the underlying agents do not need to concern themselves with the business logic; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import smooth_l1_loss\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "from maro.rl import AgentManager, Agent, AgentParameters, LearningModel, MLPDecisionLayers, DQN, DQNHyperParams, \\\n",
    "    ExperienceInfoKey\n",
    "\n",
    "num_actions = 21\n",
    "model_config = {\"hidden_dims\": [256, 128, 64], \"output_dim\": num_actions, \"dropout_p\": 0.0}\n",
    "optimizer_config = {\"lr\": 0.05}\n",
    "dqn_config = {\"num_actions\": num_actions, \"replace_target_frequency\": 5, \"tau\": 0.1}\n",
    "training_config = {\"min_experiences_to_train\": 1024, \"samplers\": [(lambda d: d[ExperienceInfoKey.TD_ERROR], 128)],\n",
    "                   \"num_steps\": 10}\n",
    "\n",
    "\n",
    "class DQNAgentManager(AgentManager):\n",
    "    def _assemble_agents(self):\n",
    "        agent_params = AgentParameters(**training_config)\n",
    "        for agent_id in self._agent_id_list:\n",
    "            eval_model = LearningModel(decision_layers=MLPDecisionLayers(name=f'{agent_id}.policy',\n",
    "                                                                         input_dim=self._state_shaper.dim,\n",
    "                                                                         **model_config)\n",
    "                                       )\n",
    "\n",
    "            algorithm = DQN(model_dict={\"eval\": eval_model}, optimizer_opt=(RMSprop, optimizer_config),\n",
    "                            loss_func_dict={\"eval\": smooth_l1_loss}, hyper_params=DQNHyperParams(**dqn_config))\n",
    "\n",
    "            self._agent_dict[agent_id] = Agent(name=agent_id, algorithm=algorithm, params=agent_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main loop\n",
    "\n",
    "The code below demonstrates the typical structure of a program using MARO. One starts by creating an environment. Next, shapers and an explorer are created and an agent manager is created by loading these components. The creation of the agent manager also assembles all agents under the hood. Because the code is for the single-host mode, the agent manager mode is set to TRAIN_INFERENCE. Next, an actor is created to wrap the env and agent manager, and a learner is created to wrap the same agent manager and the actor. Finally, the task is started by calling the learner's train_test() method.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.simulator import Env\n",
    "from maro.rl import SimpleLearner, SimpleActor, AgentMode, KStepRewardShaper, TwoPhaseLinearExplorer\n",
    "from maro.utils import Logger, convert_dottable\n",
    "\n",
    "\n",
    "total_episodes = 100\n",
    "\n",
    "env = Env(scenario=\"ecr\", topology=\"toy.5p_ssddd_l0.0\", durations=1120)\n",
    "agent_id_list = [str(agent_id) for agent_id in env.agent_idx_list]\n",
    "state_shaper = ECRStateShaper(look_back=7, max_ports_downstream=2,\n",
    "                              port_attributes=[\"empty\",\"full\",\"on_shipper\",\"on_consignee\",\"booking\",\"shortage\",\"fulfillment\"],\n",
    "                              vessel_attributes=[\"empty\",\"full\", \"remaining_space\"]\n",
    "                             )\n",
    "action_shaper = ECRActionShaper(action_space=list(np.linspace(-1.0, 1.0, num_actions)))\n",
    "reward_shaper = ECRRewardShaper(agent_id_list=agent_id_list, time_window=100, fulfillment_factor=1.0,\n",
    "                                shortage_factor=1.0, time_decay_factor=0.97)\n",
    "explorer = TwoPhaseLinearExplorer(agent_id_list, total_episodes, \n",
    "                                  epsilon_range_dict={\"_all_\": (.0, .4)},\n",
    "                                  split_point_dict={\"_all_\": (.5, .8)}\n",
    "                                 )\n",
    "agent_manager = DQNAgentManager(name=\"ecr_learner\",\n",
    "                                mode=AgentMode.TRAIN_INFERENCE,\n",
    "                                agent_id_list=agent_id_list,\n",
    "                                state_shaper=state_shaper,\n",
    "                                action_shaper=action_shaper,\n",
    "                                reward_shaper=reward_shaper,\n",
    "                                explorer=explorer)\n",
    "learner = SimpleLearner(trainable_agents=agent_manager,\n",
    "                        actor=SimpleActor(env=env, inference_agents=agent_manager),\n",
    "                        logger=Logger(\"single_host_ecr_learner\", auto_timestamp=False))\n",
    "\n",
    "learner.train_test(total_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
