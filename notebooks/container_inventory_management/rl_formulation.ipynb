{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "This notebook demonstrates how to use MARO's reinforcement learning (RL) toolkit to solve the container inventory management ([CIM](https://maro.readthedocs.io/en/latest/scenarios/container_inventory_management.html)) problem. It is formalized as a multi-agent reinforcement learning problem, where each port acts as a decision agent. The agents take actions independently, e.g., loading containers to vessels or discharging containers from vessels.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [State Shaper](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#shapers)\n",
    "\n",
    "State shaper converts the environment observation to the model input state which includes temporal and spatial information. For this scenario, the model input state includes: \n",
    "\n",
    "- Temporal information, including the past week's information of ports and vessels, such as shortage on port and remaining space on vessel. \n",
    "\n",
    "- Spatial information, it including the related downstream port features.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maro.rl import Shaper\n",
    "\n",
    "\n",
    "PORT_ATTRIBUTES = [\"empty\", \"full\", \"on_shipper\", \"on_consignee\", \"booking\", \"shortage\", \"fulfillment\"]\n",
    "VESSEL_ATTRIBUTES = [\"empty\", \"full\", \"remaining_space\"]\n",
    "\n",
    "\n",
    "class CIMStateShaper(Shaper):\n",
    "    def __init__(self, *, look_back, max_ports_downstream):\n",
    "        super().__init__()\n",
    "        self._look_back = look_back\n",
    "        self._max_ports_downstream = max_ports_downstream\n",
    "        self._dim = (look_back + 1) * (max_ports_downstream + 1) * len(PORT_ATTRIBUTES) + len(VESSEL_ATTRIBUTES)\n",
    "\n",
    "    def __call__(self, decision_event, snapshot_list):\n",
    "        tick, port_idx, vessel_idx = decision_event.tick, decision_event.port_idx, decision_event.vessel_idx\n",
    "        ticks = [tick - rt for rt in range(self._look_back - 1)]\n",
    "        future_port_idx_list = snapshot_list[\"vessels\"][tick: vessel_idx: 'future_stop_list'].astype('int')\n",
    "        port_features = snapshot_list[\"ports\"][ticks: [port_idx] + list(future_port_idx_list): PORT_ATTRIBUTES]\n",
    "        vessel_features = snapshot_list[\"vessels\"][tick: vessel_idx: VESSEL_ATTRIBUTES]\n",
    "        state = np.concatenate((port_features, vessel_features))\n",
    "        return str(port_idx), state\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim\n",
    "    \n",
    "# Create a state shaper\n",
    "state_shaper = CIMStateShaper(look_back=7, max_ports_downstream=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Action Shaper](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#shapers)\n",
    "\n",
    "Action shaper is used to convert an agent's model output to an environment executable action. For this specific scenario, the output is a discrete index that corresponds to a percentage indicating the fraction of containers to be loaded to or discharged from the arriving vessel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.simulator.scenarios.cim.common import Action\n",
    "\n",
    "\n",
    "class CIMActionShaper(Shaper):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__()\n",
    "        self._action_space = action_space\n",
    "        self._zero_action_index = action_space.index(0)\n",
    "\n",
    "    def __call__(self, model_action, decision_event, snapshot_list):\n",
    "        assert 0 <= model_action < len(self._action_space)\n",
    "        \n",
    "        scope = decision_event.action_scope\n",
    "        tick = decision_event.tick\n",
    "        port_idx = decision_event.port_idx\n",
    "        vessel_idx = decision_event.vessel_idx\n",
    "        port_empty = snapshot_list[\"ports\"][tick: port_idx: [\"empty\", \"full\", \"on_shipper\", \"on_consignee\"]][0]\n",
    "        vessel_remaining_space = snapshot_list[\"vessels\"][tick: vessel_idx: [\"empty\", \"full\", \"remaining_space\"]][2]\n",
    "        early_discharge = snapshot_list[\"vessels\"][tick:vessel_idx: \"early_discharge\"][0]\n",
    "     \n",
    "        if model_action < self._zero_action_index:\n",
    "            # The number of loaded containers must be less than the vessel's remaining space.\n",
    "            actual_action = max(round(self._action_space[model_action] * port_empty), -vessel_remaining_space)\n",
    "        elif model_action > self._zero_action_index:\n",
    "            # In the case of an early discharge event, we need to subtract the early discharge amount from the expected \n",
    "            # discharge quote.   \n",
    "            plan_action = self._action_space[model_action] * (scope.discharge + early_discharge) - early_discharge\n",
    "            actual_action = round(plan_action) if plan_action > 0 else round(self._action_space[model_action] * scope.discharge)\n",
    "        else:\n",
    "            actual_action = 0\n",
    "\n",
    "        return Action(vessel_idx, port_idx, actual_action)\n",
    "    \n",
    "# Create an action shaper\n",
    "NUM_ACTIONS = 21\n",
    "action_shaper = CIMActionShaper(action_space=list(np.linspace(-1.0, 1.0, NUM_ACTIONS)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Experience Shaper](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#shapers)\n",
    "\n",
    "Experience shaper is used to convert an episode trajectory to trainable experiences for RL agents. For this specific scenario, the reward is a linear combination of fulfillment and shortage in a limited time window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class TruncatedExperienceShaper(Shaper):\n",
    "    def __init__(\n",
    "        self, *, time_window: int, time_decay_factor: float, fulfillment_factor: float, shortage_factor: float\n",
    "    ):\n",
    "        super().__init__(reward_func=None)\n",
    "        self._time_window = time_window\n",
    "        self._time_decay_factor = time_decay_factor\n",
    "        self._fulfillment_factor = fulfillment_factor\n",
    "        self._shortage_factor = shortage_factor\n",
    "\n",
    "    def __call__(self, trajectory, snapshot_list):\n",
    "        experiences_by_agent = defaultdict(lambda: defaultdict(list))\n",
    "        states = trajectory[\"state\"]\n",
    "        actions = trajectory[\"action\"]\n",
    "        agent_ids = trajectory[\"agent_id\"]\n",
    "        events = trajectory[\"event\"]\n",
    "        for i in range(len(states) - 1):\n",
    "            experiences = experiences_by_agent[agent_ids[i]]\n",
    "            experiences[\"state\"].append(states[i])\n",
    "            experiences[\"action\"].append(actions[i])\n",
    "            experiences[\"reward\"].append(self._compute_reward(events[i], snapshot_list))\n",
    "            experiences[\"next_state\"].append(states[i + 1])\n",
    "\n",
    "        return dict(experiences_by_agent)\n",
    "\n",
    "    def _compute_reward(self, decision_event, snapshot_list):\n",
    "        start_tick = decision_event.tick + 1\n",
    "        end_tick = decision_event.tick + self._time_window\n",
    "        ticks = list(range(start_tick, end_tick))\n",
    "\n",
    "        # calculate tc reward\n",
    "        future_fulfillment = snapshot_list[\"ports\"][ticks::\"fulfillment\"]\n",
    "        future_shortage = snapshot_list[\"ports\"][ticks::\"shortage\"]\n",
    "        decay_list = [\n",
    "            self._time_decay_factor ** i for i in range(end_tick - start_tick)\n",
    "            for _ in range(future_fulfillment.shape[0] // (end_tick - start_tick))\n",
    "        ]\n",
    "\n",
    "        tot_fulfillment = np.dot(future_fulfillment, decay_list)\n",
    "        tot_shortage = np.dot(future_shortage, decay_list)\n",
    "\n",
    "        return np.float32(self._fulfillment_factor * tot_fulfillment - self._shortage_factor * tot_shortage)\n",
    "    \n",
    "# Create an experience shaper\n",
    "experience_shaper = TruncatedExperienceShaper(time_window=100, fulfillment_factor=1.0, shortage_factor=1.0, time_decay_factor=0.97)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Agent](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#agent)\n",
    "\n",
    "For this scenario, the agent is the algorithmic abstraction of a port. We choose DQN as our underlying learning algorithm with a TD-error-based sampling mechanism.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.nn.functional import smooth_l1_loss\n",
    "from torch.optim import RMSprop\n",
    "\n",
    "from maro.rl import DQN, DQNConfig, FullyConnectedBlock, OptimOption, SimpleMultiHeadModel, SimpleStore\n",
    "from maro.utils import set_seeds\n",
    "\n",
    "\n",
    "def create_dqn_agents(agent_id_list):\n",
    "    set_seeds(64)  # for reproducibility\n",
    "    agent_dict = {}\n",
    "    for agent_id in agent_id_list:\n",
    "        q_net = FullyConnectedBlock(\n",
    "            input_dim=state_shaper.dim,\n",
    "            hidden_dims=[256, 128, 64],\n",
    "            output_dim=NUM_ACTIONS,\n",
    "            activation=nn.LeakyReLU,\n",
    "            is_head=True,\n",
    "            batch_norm=True, \n",
    "            softmax=False,\n",
    "            skip_connection=False,\n",
    "            dropout_p=.0\n",
    "        )\n",
    "        \n",
    "        learning_model = SimpleMultiHeadModel(\n",
    "            q_net, optim_option=OptimOption(optim_cls=RMSprop, optim_params={\"lr\": 0.05})\n",
    "        )\n",
    "        agent_dict[agent_id] = DQN(\n",
    "            agent_id, \n",
    "            learning_model, \n",
    "            config=DQNConfig(\n",
    "                reward_discount=.0, \n",
    "                min_exp_to_train=1024,\n",
    "                num_batches=10,\n",
    "                batch_size=128, \n",
    "                target_update_freq=5, \n",
    "                tau=0.1, \n",
    "                is_double=True, \n",
    "                per_sample_td_error=True,\n",
    "                loss_cls=nn.SmoothL1Loss\n",
    "            )\n",
    "        )\n",
    "\n",
    "    return agent_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Agent Manager](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#agent-manager)\n",
    "\n",
    "The complexities of the environment can be isolated from the learning algorithm by using an AgentManager to manage individual agents. We define a function to create the agents and an agent manager class that implements the ``train`` method where the newly obtained experiences are stored in the agents' experience pools before training, in accordance with the DQN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.rl import AbsAgentManager\n",
    "\n",
    "\n",
    "class DQNAgentManager(AbsAgentManager):\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent,\n",
    "        state_shaper: CIMStateShaper,\n",
    "        action_shaper: CIMActionShaper,\n",
    "        experience_shaper: TruncatedExperienceShaper\n",
    "    ):\n",
    "        super().__init__(\n",
    "            agent,\n",
    "            state_shaper=state_shaper,\n",
    "            action_shaper=action_shaper,\n",
    "            experience_shaper=experience_shaper\n",
    "        )\n",
    "        # Data structure to temporarily store the trajectory\n",
    "        self._trajectory = defaultdict(list)\n",
    "\n",
    "    def choose_action(self, decision_event, snapshot_list):\n",
    "        agent_id, model_state = self._state_shaper(decision_event, snapshot_list)\n",
    "        action = self.agent[agent_id].choose_action(model_state)\n",
    "        self._trajectory[\"state\"].append(model_state)\n",
    "        self._trajectory[\"agent_id\"].append(agent_id)\n",
    "        self._trajectory[\"event\"].append(decision_event)\n",
    "        self._trajectory[\"action\"].append(action)\n",
    "        return self._action_shaper(action, decision_event, snapshot_list)\n",
    "\n",
    "    def train(self, experiences_by_agent):\n",
    "        # store experiences for each agent\n",
    "        for agent_id, exp in experiences_by_agent.items():\n",
    "            exp.update({\"loss\": [1e8] * len(list(exp.values())[0])})\n",
    "            self.agent[agent_id].store_experiences(exp)\n",
    "\n",
    "        for agent in self.agent.values():\n",
    "            agent.train()\n",
    "\n",
    "    def on_env_feedback(self, metrics):\n",
    "        self._trajectory[\"metrics\"].append(metrics)\n",
    "\n",
    "    def post_process(self, snapshot_list):\n",
    "        experiences = self._experience_shaper(self._trajectory, snapshot_list)\n",
    "        self._trajectory.clear()\n",
    "        self._state_shaper.reset()\n",
    "        self._action_shaper.reset()\n",
    "        self._experience_shaper.reset()\n",
    "        return experiences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Loop with [Actor and Learner](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#learner-and-actor)\n",
    "\n",
    "This code cell demonstrates the typical workflow of a learning policy's interaction with a MARO environment. \n",
    "\n",
    "- Initialize an environment with specific scenario and topology parameters. \n",
    "\n",
    "- Define scenario-specific components, e.g. shapers. \n",
    "\n",
    "- Create agents and an agent manager. \n",
    "\n",
    "- Create an actor and a learner to start the training process in which the agent manager interacts with the environment for collecting experiences and updating policies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03:58:15 | cim_learner | INFO | ep 0 - performance: {'order_requirements': 2240000, 'container_shortage': 1352136, 'operation_number': 3254760}, exploration_params: {'epsilon': 0.4}\n",
      "03:58:20 | cim_learner | INFO | ep 1 - performance: {'order_requirements': 2240000, 'container_shortage': 1249849, 'operation_number': 3426101}, exploration_params: {'epsilon': 0.39840000000000003}\n",
      "03:58:25 | cim_learner | INFO | ep 2 - performance: {'order_requirements': 2240000, 'container_shortage': 1174857, 'operation_number': 3816050}, exploration_params: {'epsilon': 0.39680000000000004}\n",
      "03:58:30 | cim_learner | INFO | ep 3 - performance: {'order_requirements': 2240000, 'container_shortage': 1168029, 'operation_number': 3783409}, exploration_params: {'epsilon': 0.39520000000000005}\n",
      "03:58:35 | cim_learner | INFO | ep 4 - performance: {'order_requirements': 2240000, 'container_shortage': 1478014, 'operation_number': 3503012}, exploration_params: {'epsilon': 0.39360000000000006}\n",
      "03:58:40 | cim_learner | INFO | ep 5 - performance: {'order_requirements': 2240000, 'container_shortage': 1450497, 'operation_number': 4235336.0}, exploration_params: {'epsilon': 0.39200000000000007}\n",
      "03:58:45 | cim_learner | INFO | ep 6 - performance: {'order_requirements': 2240000, 'container_shortage': 1250538, 'operation_number': 3681417}, exploration_params: {'epsilon': 0.3904000000000001}\n",
      "03:58:50 | cim_learner | INFO | ep 7 - performance: {'order_requirements': 2240000, 'container_shortage': 1923270, 'operation_number': 2793886}, exploration_params: {'epsilon': 0.3888000000000001}\n",
      "03:58:56 | cim_learner | INFO | ep 8 - performance: {'order_requirements': 2240000, 'container_shortage': 1621199, 'operation_number': 2655534}, exploration_params: {'epsilon': 0.3872000000000001}\n",
      "03:59:01 | cim_learner | INFO | ep 9 - performance: {'order_requirements': 2240000, 'container_shortage': 1185373, 'operation_number': 3303864}, exploration_params: {'epsilon': 0.3856000000000001}\n",
      "03:59:07 | cim_learner | INFO | ep 10 - performance: {'order_requirements': 2240000, 'container_shortage': 720509, 'operation_number': 3923254}, exploration_params: {'epsilon': 0.3840000000000001}\n",
      "03:59:12 | cim_learner | INFO | ep 11 - performance: {'order_requirements': 2240000, 'container_shortage': 604046, 'operation_number': 4055816}, exploration_params: {'epsilon': 0.38240000000000013}\n",
      "03:59:17 | cim_learner | INFO | ep 12 - performance: {'order_requirements': 2240000, 'container_shortage': 721395, 'operation_number': 4088165}, exploration_params: {'epsilon': 0.38080000000000014}\n",
      "03:59:23 | cim_learner | INFO | ep 13 - performance: {'order_requirements': 2240000, 'container_shortage': 569133, 'operation_number': 5024948}, exploration_params: {'epsilon': 0.37920000000000015}\n",
      "03:59:28 | cim_learner | INFO | ep 14 - performance: {'order_requirements': 2240000, 'container_shortage': 672845, 'operation_number': 4619933}, exploration_params: {'epsilon': 0.37760000000000016}\n",
      "03:59:34 | cim_learner | INFO | ep 15 - performance: {'order_requirements': 2240000, 'container_shortage': 899736, 'operation_number': 4688569}, exploration_params: {'epsilon': 0.37600000000000017}\n",
      "03:59:39 | cim_learner | INFO | ep 16 - performance: {'order_requirements': 2240000, 'container_shortage': 950907, 'operation_number': 4453843}, exploration_params: {'epsilon': 0.3744000000000002}\n",
      "03:59:44 | cim_learner | INFO | ep 17 - performance: {'order_requirements': 2240000, 'container_shortage': 642622, 'operation_number': 5087980}, exploration_params: {'epsilon': 0.3728000000000002}\n",
      "03:59:49 | cim_learner | INFO | ep 18 - performance: {'order_requirements': 2240000, 'container_shortage': 804926, 'operation_number': 4627842}, exploration_params: {'epsilon': 0.3712000000000002}\n",
      "03:59:55 | cim_learner | INFO | ep 19 - performance: {'order_requirements': 2240000, 'container_shortage': 955385, 'operation_number': 4955868}, exploration_params: {'epsilon': 0.3696000000000002}\n",
      "04:00:00 | cim_learner | INFO | ep 20 - performance: {'order_requirements': 2240000, 'container_shortage': 829930, 'operation_number': 5148070}, exploration_params: {'epsilon': 0.3680000000000002}\n",
      "04:00:06 | cim_learner | INFO | ep 21 - performance: {'order_requirements': 2240000, 'container_shortage': 644572, 'operation_number': 5105741}, exploration_params: {'epsilon': 0.3664000000000002}\n",
      "04:00:11 | cim_learner | INFO | ep 22 - performance: {'order_requirements': 2240000, 'container_shortage': 555530, 'operation_number': 4839572}, exploration_params: {'epsilon': 0.36480000000000024}\n",
      "04:00:16 | cim_learner | INFO | ep 23 - performance: {'order_requirements': 2240000, 'container_shortage': 1031378, 'operation_number': 3728440}, exploration_params: {'epsilon': 0.36320000000000024}\n",
      "04:00:22 | cim_learner | INFO | ep 24 - performance: {'order_requirements': 2240000, 'container_shortage': 723926, 'operation_number': 5235602}, exploration_params: {'epsilon': 0.36160000000000025}\n",
      "04:00:28 | cim_learner | INFO | ep 25 - performance: {'order_requirements': 2240000, 'container_shortage': 676156, 'operation_number': 5142291}, exploration_params: {'epsilon': 0.36000000000000026}\n",
      "04:00:33 | cim_learner | INFO | ep 26 - performance: {'order_requirements': 2240000, 'container_shortage': 842840, 'operation_number': 5028770}, exploration_params: {'epsilon': 0.3584000000000003}\n",
      "04:00:39 | cim_learner | INFO | ep 27 - performance: {'order_requirements': 2240000, 'container_shortage': 865620, 'operation_number': 4766610}, exploration_params: {'epsilon': 0.3568000000000003}\n",
      "04:00:44 | cim_learner | INFO | ep 28 - performance: {'order_requirements': 2240000, 'container_shortage': 794776, 'operation_number': 5052284}, exploration_params: {'epsilon': 0.3552000000000003}\n",
      "04:00:50 | cim_learner | INFO | ep 29 - performance: {'order_requirements': 2240000, 'container_shortage': 699853, 'operation_number': 5152245}, exploration_params: {'epsilon': 0.3536000000000003}\n",
      "04:00:55 | cim_learner | INFO | ep 30 - performance: {'order_requirements': 2240000, 'container_shortage': 616300, 'operation_number': 4678601}, exploration_params: {'epsilon': 0.3520000000000003}\n",
      "04:01:01 | cim_learner | INFO | ep 31 - performance: {'order_requirements': 2240000, 'container_shortage': 728820, 'operation_number': 4974799}, exploration_params: {'epsilon': 0.3504000000000003}\n",
      "04:01:07 | cim_learner | INFO | ep 32 - performance: {'order_requirements': 2240000, 'container_shortage': 727185, 'operation_number': 4755672}, exploration_params: {'epsilon': 0.34880000000000033}\n",
      "04:01:12 | cim_learner | INFO | ep 33 - performance: {'order_requirements': 2240000, 'container_shortage': 727381, 'operation_number': 4873634}, exploration_params: {'epsilon': 0.34720000000000034}\n",
      "04:01:18 | cim_learner | INFO | ep 34 - performance: {'order_requirements': 2240000, 'container_shortage': 679647, 'operation_number': 4624782}, exploration_params: {'epsilon': 0.34560000000000035}\n",
      "04:01:23 | cim_learner | INFO | ep 35 - performance: {'order_requirements': 2240000, 'container_shortage': 625849, 'operation_number': 4947661}, exploration_params: {'epsilon': 0.34400000000000036}\n",
      "04:01:28 | cim_learner | INFO | ep 36 - performance: {'order_requirements': 2240000, 'container_shortage': 706626, 'operation_number': 4546677}, exploration_params: {'epsilon': 0.34240000000000037}\n",
      "04:01:34 | cim_learner | INFO | ep 37 - performance: {'order_requirements': 2240000, 'container_shortage': 560302, 'operation_number': 4578177}, exploration_params: {'epsilon': 0.3408000000000004}\n",
      "04:01:39 | cim_learner | INFO | ep 38 - performance: {'order_requirements': 2240000, 'container_shortage': 774175, 'operation_number': 5005750}, exploration_params: {'epsilon': 0.3392000000000004}\n",
      "04:01:45 | cim_learner | INFO | ep 39 - performance: {'order_requirements': 2240000, 'container_shortage': 584020, 'operation_number': 4787339}, exploration_params: {'epsilon': 0.3376000000000004}\n",
      "04:01:50 | cim_learner | INFO | ep 40 - performance: {'order_requirements': 2240000, 'container_shortage': 576394, 'operation_number': 4720419}, exploration_params: {'epsilon': 0.3360000000000004}\n",
      "04:01:56 | cim_learner | INFO | ep 41 - performance: {'order_requirements': 2240000, 'container_shortage': 702115, 'operation_number': 4928421}, exploration_params: {'epsilon': 0.3344000000000004}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04:02:01 | cim_learner | INFO | ep 42 - performance: {'order_requirements': 2240000, 'container_shortage': 822003, 'operation_number': 5127494}, exploration_params: {'epsilon': 0.33280000000000043}\n",
      "04:02:07 | cim_learner | INFO | ep 43 - performance: {'order_requirements': 2240000, 'container_shortage': 676856, 'operation_number': 4361711}, exploration_params: {'epsilon': 0.33120000000000044}\n",
      "04:02:12 | cim_learner | INFO | ep 44 - performance: {'order_requirements': 2240000, 'container_shortage': 669537, 'operation_number': 5291246}, exploration_params: {'epsilon': 0.32960000000000045}\n",
      "04:02:18 | cim_learner | INFO | ep 45 - performance: {'order_requirements': 2240000, 'container_shortage': 569000, 'operation_number': 4652232}, exploration_params: {'epsilon': 0.32800000000000046}\n",
      "04:02:23 | cim_learner | INFO | ep 46 - performance: {'order_requirements': 2240000, 'container_shortage': 604969, 'operation_number': 5123438}, exploration_params: {'epsilon': 0.32640000000000047}\n",
      "04:02:29 | cim_learner | INFO | ep 47 - performance: {'order_requirements': 2240000, 'container_shortage': 557511, 'operation_number': 4832546}, exploration_params: {'epsilon': 0.3248000000000005}\n",
      "04:02:34 | cim_learner | INFO | ep 48 - performance: {'order_requirements': 2240000, 'container_shortage': 661551, 'operation_number': 4916774}, exploration_params: {'epsilon': 0.3232000000000005}\n",
      "04:02:40 | cim_learner | INFO | ep 49 - performance: {'order_requirements': 2240000, 'container_shortage': 678443, 'operation_number': 4797744}, exploration_params: {'epsilon': 0.3216000000000005}\n",
      "04:02:45 | cim_learner | INFO | ep 50 - performance: {'order_requirements': 2240000, 'container_shortage': 647478, 'operation_number': 4983993}, exploration_params: {'epsilon': 0.3200000000000005}\n",
      "04:02:51 | cim_learner | INFO | ep 51 - performance: {'order_requirements': 2240000, 'container_shortage': 436833, 'operation_number': 4411942}, exploration_params: {'epsilon': 0.3134693877551025}\n",
      "04:02:56 | cim_learner | INFO | ep 52 - performance: {'order_requirements': 2240000, 'container_shortage': 626074, 'operation_number': 4618660}, exploration_params: {'epsilon': 0.30693877551020454}\n",
      "04:03:02 | cim_learner | INFO | ep 53 - performance: {'order_requirements': 2240000, 'container_shortage': 708019, 'operation_number': 4538794}, exploration_params: {'epsilon': 0.30040816326530656}\n",
      "04:03:08 | cim_learner | INFO | ep 54 - performance: {'order_requirements': 2240000, 'container_shortage': 704531, 'operation_number': 5035620}, exploration_params: {'epsilon': 0.2938775510204086}\n",
      "04:03:14 | cim_learner | INFO | ep 55 - performance: {'order_requirements': 2240000, 'container_shortage': 675365, 'operation_number': 4744667}, exploration_params: {'epsilon': 0.2873469387755106}\n",
      "04:03:19 | cim_learner | INFO | ep 56 - performance: {'order_requirements': 2240000, 'container_shortage': 536896, 'operation_number': 4664484}, exploration_params: {'epsilon': 0.2808163265306126}\n",
      "04:03:25 | cim_learner | INFO | ep 57 - performance: {'order_requirements': 2240000, 'container_shortage': 517742, 'operation_number': 4515999}, exploration_params: {'epsilon': 0.27428571428571463}\n",
      "04:03:31 | cim_learner | INFO | ep 58 - performance: {'order_requirements': 2240000, 'container_shortage': 495825, 'operation_number': 4592775}, exploration_params: {'epsilon': 0.26775510204081665}\n",
      "04:03:37 | cim_learner | INFO | ep 59 - performance: {'order_requirements': 2240000, 'container_shortage': 485726, 'operation_number': 4586843}, exploration_params: {'epsilon': 0.26122448979591867}\n",
      "04:03:42 | cim_learner | INFO | ep 60 - performance: {'order_requirements': 2240000, 'container_shortage': 350307, 'operation_number': 4856216}, exploration_params: {'epsilon': 0.2546938775510207}\n",
      "04:03:48 | cim_learner | INFO | ep 61 - performance: {'order_requirements': 2240000, 'container_shortage': 440101, 'operation_number': 4511357}, exploration_params: {'epsilon': 0.24816326530612273}\n",
      "04:03:54 | cim_learner | INFO | ep 62 - performance: {'order_requirements': 2240000, 'container_shortage': 366995, 'operation_number': 4489751}, exploration_params: {'epsilon': 0.24163265306122478}\n",
      "04:04:00 | cim_learner | INFO | ep 63 - performance: {'order_requirements': 2240000, 'container_shortage': 407148, 'operation_number': 4229339}, exploration_params: {'epsilon': 0.23510204081632682}\n",
      "04:04:05 | cim_learner | INFO | ep 64 - performance: {'order_requirements': 2240000, 'container_shortage': 490311, 'operation_number': 4231013}, exploration_params: {'epsilon': 0.22857142857142887}\n",
      "04:04:11 | cim_learner | INFO | ep 65 - performance: {'order_requirements': 2240000, 'container_shortage': 495735, 'operation_number': 4084791}, exploration_params: {'epsilon': 0.22204081632653092}\n",
      "04:04:17 | cim_learner | INFO | ep 66 - performance: {'order_requirements': 2240000, 'container_shortage': 531423, 'operation_number': 4125375}, exploration_params: {'epsilon': 0.21551020408163296}\n",
      "04:04:23 | cim_learner | INFO | ep 67 - performance: {'order_requirements': 2240000, 'container_shortage': 530556, 'operation_number': 4015393}, exploration_params: {'epsilon': 0.208979591836735}\n",
      "04:04:29 | cim_learner | INFO | ep 68 - performance: {'order_requirements': 2240000, 'container_shortage': 271291, 'operation_number': 4631212}, exploration_params: {'epsilon': 0.20244897959183705}\n",
      "04:04:35 | cim_learner | INFO | ep 69 - performance: {'order_requirements': 2240000, 'container_shortage': 355445, 'operation_number': 4429670}, exploration_params: {'epsilon': 0.1959183673469391}\n",
      "04:04:41 | cim_learner | INFO | ep 70 - performance: {'order_requirements': 2240000, 'container_shortage': 358595, 'operation_number': 4645877}, exploration_params: {'epsilon': 0.18938775510204114}\n",
      "04:04:47 | cim_learner | INFO | ep 71 - performance: {'order_requirements': 2240000, 'container_shortage': 269327, 'operation_number': 4763005}, exploration_params: {'epsilon': 0.1828571428571432}\n",
      "04:04:52 | cim_learner | INFO | ep 72 - performance: {'order_requirements': 2240000, 'container_shortage': 252523, 'operation_number': 4390522}, exploration_params: {'epsilon': 0.17632653061224524}\n",
      "04:04:59 | cim_learner | INFO | ep 73 - performance: {'order_requirements': 2240000, 'container_shortage': 224338, 'operation_number': 4651297}, exploration_params: {'epsilon': 0.16979591836734728}\n",
      "04:05:04 | cim_learner | INFO | ep 74 - performance: {'order_requirements': 2240000, 'container_shortage': 419598, 'operation_number': 4577744}, exploration_params: {'epsilon': 0.16326530612244933}\n",
      "04:05:10 | cim_learner | INFO | ep 75 - performance: {'order_requirements': 2240000, 'container_shortage': 319832, 'operation_number': 4523035}, exploration_params: {'epsilon': 0.15673469387755137}\n",
      "04:05:16 | cim_learner | INFO | ep 76 - performance: {'order_requirements': 2240000, 'container_shortage': 291227, 'operation_number': 4569509}, exploration_params: {'epsilon': 0.15020408163265342}\n",
      "04:05:22 | cim_learner | INFO | ep 77 - performance: {'order_requirements': 2240000, 'container_shortage': 235975, 'operation_number': 4438580}, exploration_params: {'epsilon': 0.14367346938775546}\n",
      "04:05:28 | cim_learner | INFO | ep 78 - performance: {'order_requirements': 2240000, 'container_shortage': 209720, 'operation_number': 4472226}, exploration_params: {'epsilon': 0.1371428571428575}\n",
      "04:05:34 | cim_learner | INFO | ep 79 - performance: {'order_requirements': 2240000, 'container_shortage': 203721, 'operation_number': 4475482}, exploration_params: {'epsilon': 0.13061224489795956}\n",
      "04:05:40 | cim_learner | INFO | ep 80 - performance: {'order_requirements': 2240000, 'container_shortage': 215189, 'operation_number': 4290320}, exploration_params: {'epsilon': 0.1240816326530616}\n",
      "04:05:45 | cim_learner | INFO | ep 81 - performance: {'order_requirements': 2240000, 'container_shortage': 203791, 'operation_number': 4321789}, exploration_params: {'epsilon': 0.11755102040816365}\n",
      "04:05:51 | cim_learner | INFO | ep 82 - performance: {'order_requirements': 2240000, 'container_shortage': 230472, 'operation_number': 4216193}, exploration_params: {'epsilon': 0.1110204081632657}\n",
      "04:05:57 | cim_learner | INFO | ep 83 - performance: {'order_requirements': 2240000, 'container_shortage': 148354, 'operation_number': 4274694}, exploration_params: {'epsilon': 0.10448979591836774}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04:06:03 | cim_learner | INFO | ep 84 - performance: {'order_requirements': 2240000, 'container_shortage': 196658, 'operation_number': 4234519}, exploration_params: {'epsilon': 0.09795918367346979}\n",
      "04:06:09 | cim_learner | INFO | ep 85 - performance: {'order_requirements': 2240000, 'container_shortage': 117587, 'operation_number': 4409388}, exploration_params: {'epsilon': 0.09142857142857183}\n",
      "04:06:14 | cim_learner | INFO | ep 86 - performance: {'order_requirements': 2240000, 'container_shortage': 129900, 'operation_number': 4370451}, exploration_params: {'epsilon': 0.08489795918367388}\n",
      "04:06:20 | cim_learner | INFO | ep 87 - performance: {'order_requirements': 2240000, 'container_shortage': 150391, 'operation_number': 4372565}, exploration_params: {'epsilon': 0.07836734693877592}\n",
      "04:06:27 | cim_learner | INFO | ep 88 - performance: {'order_requirements': 2240000, 'container_shortage': 240991, 'operation_number': 4263189}, exploration_params: {'epsilon': 0.07183673469387797}\n",
      "04:06:32 | cim_learner | INFO | ep 89 - performance: {'order_requirements': 2240000, 'container_shortage': 108313, 'operation_number': 4400742}, exploration_params: {'epsilon': 0.06530612244898001}\n",
      "04:06:38 | cim_learner | INFO | ep 90 - performance: {'order_requirements': 2240000, 'container_shortage': 153466, 'operation_number': 4211634}, exploration_params: {'epsilon': 0.05877551020408205}\n",
      "04:06:44 | cim_learner | INFO | ep 91 - performance: {'order_requirements': 2240000, 'container_shortage': 144307, 'operation_number': 4295363}, exploration_params: {'epsilon': 0.05224489795918409}\n",
      "04:06:50 | cim_learner | INFO | ep 92 - performance: {'order_requirements': 2240000, 'container_shortage': 135912, 'operation_number': 4270395}, exploration_params: {'epsilon': 0.04571428571428613}\n",
      "04:06:56 | cim_learner | INFO | ep 93 - performance: {'order_requirements': 2240000, 'container_shortage': 130515, 'operation_number': 4309926}, exploration_params: {'epsilon': 0.03918367346938817}\n",
      "04:07:02 | cim_learner | INFO | ep 94 - performance: {'order_requirements': 2240000, 'container_shortage': 129339, 'operation_number': 4209056}, exploration_params: {'epsilon': 0.03265306122449021}\n",
      "04:07:08 | cim_learner | INFO | ep 95 - performance: {'order_requirements': 2240000, 'container_shortage': 134269, 'operation_number': 4254431}, exploration_params: {'epsilon': 0.026122448979592247}\n",
      "04:07:14 | cim_learner | INFO | ep 96 - performance: {'order_requirements': 2240000, 'container_shortage': 104881, 'operation_number': 4244267}, exploration_params: {'epsilon': 0.019591836734694286}\n",
      "04:07:20 | cim_learner | INFO | ep 97 - performance: {'order_requirements': 2240000, 'container_shortage': 89076, 'operation_number': 4300194}, exploration_params: {'epsilon': 0.013061224489796327}\n",
      "04:07:26 | cim_learner | INFO | ep 98 - performance: {'order_requirements': 2240000, 'container_shortage': 94324, 'operation_number': 4267081}, exploration_params: {'epsilon': 0.006530612244898367}\n",
      "04:07:32 | cim_learner | INFO | ep 99 - performance: {'order_requirements': 2240000, 'container_shortage': 88931, 'operation_number': 4267048}, exploration_params: {'epsilon': 4.0766001685454967e-16}\n"
     ]
    }
   ],
   "source": [
    "from maro.simulator import Env\n",
    "from maro.rl import SimpleActor, SimpleLearner, TwoPhaseLinearParameterScheduler\n",
    "from maro.utils import LogFormat, Logger\n",
    "\n",
    "# Step 1: initialize a CIM environment for a toy dataset. \n",
    "env = Env(\"cim\", \"toy.4p_ssdd_l0.0\", durations=1120)\n",
    "agent_id_list = [str(agent_id) for agent_id in env.agent_idx_list]\n",
    "\n",
    "# Step 2: create DQN agents and an agent manager to manage them.\n",
    "agent_manager = DQNAgentManager(\n",
    "    create_dqn_agents(agent_id_list),\n",
    "    state_shaper=state_shaper,\n",
    "    action_shaper=action_shaper,\n",
    "    experience_shaper=experience_shaper\n",
    ")\n",
    "\n",
    "# Step 3: Create an actor and a learner to start the training process. \n",
    "max_episode = 100\n",
    "scheduler = TwoPhaseLinearParameterScheduler(\n",
    "    max_episode,\n",
    "    parameter_names=[\"epsilon\"],\n",
    "    split_ep=50,\n",
    "    start_values=0.4,\n",
    "    mid_values=0.32,\n",
    "    end_values=.0\n",
    ")\n",
    "\n",
    "actor = SimpleActor(env, agent_manager)\n",
    "learner = SimpleLearner(\n",
    "    agent_manager, actor, scheduler, \n",
    "    logger=Logger(\"cim_learner\", format_=LogFormat.simple, auto_timestamp=False)\n",
    ")\n",
    "\n",
    "learner.learn()\n",
    "learner.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maro",
   "language": "python",
   "name": "maro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
