{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "This notebook demonstrates the use of MARO's RL toolkit to optimize container inventory management (CIM). The scenario consists of a set of ports, each acting as a learning agent, and vessels that transfer empty containers among them. Each port must decide 1) whether to load or discharge containers when a vessel arrives and 2) how many containers to be loaded or discharged. The objective is to minimize the overall container shortage over a certain period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "from typing import Any, Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam, RMSprop\n",
    "\n",
    "from maro.rl.model import DiscreteACBasedNet, FullyConnected, VNet\n",
    "from maro.rl.policy import DiscretePolicyGradient\n",
    "from maro.rl.rl_component.rl_component_bundle import RLComponentBundle\n",
    "from maro.rl.rollout import AbsEnvSampler, CacheElement, ExpElement\n",
    "from maro.rl.training import TrainingManager\n",
    "from maro.rl.training.algorithms import PPOParams, PPOTrainer\n",
    "from maro.simulator import Env\n",
    "from maro.simulator.scenarios.cim.common import Action, ActionType, DecisionEvent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env and shaping config\n",
    "reward_shaping_conf = {\n",
    "    \"time_window\": 99,\n",
    "    \"fulfillment_factor\": 1.0,\n",
    "    \"shortage_factor\": 1.0,\n",
    "    \"time_decay\": 0.97,\n",
    "}\n",
    "state_shaping_conf = {\n",
    "    \"look_back\": 7,\n",
    "    \"max_ports_downstream\": 2,\n",
    "}\n",
    "port_attributes = [\"empty\", \"full\", \"on_shipper\", \"on_consignee\", \"booking\", \"shortage\", \"fulfillment\"]\n",
    "vessel_attributes = [\"empty\", \"full\", \"remaining_space\"]\n",
    "action_shaping_conf = {\n",
    "    \"action_space\": [(i - 10) / 10 for i in range(21)],\n",
    "    \"finite_vessel_space\": True,\n",
    "    \"has_early_discharge\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Sampler\n",
    "\n",
    "An environment sampler defines state, action and reward shaping logic so that policies can interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIMEnvSampler(AbsEnvSampler):\n",
    "    def _get_global_and_agent_state_impl(\n",
    "        self, event: DecisionEvent, tick: int = None,\n",
    "    ) -> Tuple[Union[None, np.ndarray, List[object]], Dict[Any, Union[np.ndarray, List[object]]]]:\n",
    "        tick = self._env.tick\n",
    "        vessel_snapshots, port_snapshots = self._env.snapshot_list[\"vessels\"], self._env.snapshot_list[\"ports\"]\n",
    "        port_idx, vessel_idx = event.port_idx, event.vessel_idx\n",
    "        ticks = [max(0, tick - rt) for rt in range(state_shaping_conf[\"look_back\"] - 1)]\n",
    "        future_port_list = vessel_snapshots[tick: vessel_idx: 'future_stop_list'].astype('int')\n",
    "        state = np.concatenate([\n",
    "            port_snapshots[ticks: [port_idx] + list(future_port_list): port_attributes],\n",
    "            vessel_snapshots[tick: vessel_idx: vessel_attributes]\n",
    "        ])\n",
    "        return state, {port_idx: state}\n",
    "\n",
    "    def _translate_to_env_action(\n",
    "        self, action_dict: Dict[Any, Union[np.ndarray, List[object]]], event: DecisionEvent,\n",
    "    ) -> Dict[Any, object]:\n",
    "        action_space = action_shaping_conf[\"action_space\"]\n",
    "        finite_vsl_space = action_shaping_conf[\"finite_vessel_space\"]\n",
    "        has_early_discharge = action_shaping_conf[\"has_early_discharge\"]\n",
    "\n",
    "        port_idx, model_action = list(action_dict.items()).pop()\n",
    "\n",
    "        vsl_idx, action_scope = event.vessel_idx, event.action_scope\n",
    "        vsl_snapshots = self._env.snapshot_list[\"vessels\"]\n",
    "        vsl_space = vsl_snapshots[self._env.tick:vsl_idx:vessel_attributes][2] if finite_vsl_space else float(\"inf\")\n",
    "\n",
    "        percent = abs(action_space[model_action[0]])\n",
    "        zero_action_idx = len(action_space) / 2  # index corresponding to value zero.\n",
    "        if model_action < zero_action_idx:\n",
    "            action_type = ActionType.LOAD\n",
    "            actual_action = min(round(percent * action_scope.load), vsl_space)\n",
    "        elif model_action > zero_action_idx:\n",
    "            action_type = ActionType.DISCHARGE\n",
    "            early_discharge = vsl_snapshots[self._env.tick:vsl_idx:\"early_discharge\"][0] if has_early_discharge else 0\n",
    "            plan_action = percent * (action_scope.discharge + early_discharge) - early_discharge\n",
    "            actual_action = round(plan_action) if plan_action > 0 else round(percent * action_scope.discharge)\n",
    "        else:\n",
    "            actual_action, action_type = 0, None\n",
    "\n",
    "        return {port_idx: Action(vsl_idx, int(port_idx), actual_action, action_type)}\n",
    "\n",
    "    def _get_reward(self, env_action_dict: Dict[Any, object], event: DecisionEvent, tick: int) -> Dict[Any, float]:\n",
    "        start_tick = tick + 1\n",
    "        ticks = list(range(start_tick, start_tick + reward_shaping_conf[\"time_window\"]))\n",
    "\n",
    "        # Get the ports that took actions at the given tick\n",
    "        ports = [int(port) for port in list(env_action_dict.keys())]\n",
    "        port_snapshots = self._env.snapshot_list[\"ports\"]\n",
    "        future_fulfillment = port_snapshots[ticks:ports:\"fulfillment\"].reshape(len(ticks), -1)\n",
    "        future_shortage = port_snapshots[ticks:ports:\"shortage\"].reshape(len(ticks), -1)\n",
    "\n",
    "        decay_list = [reward_shaping_conf[\"time_decay\"] ** i for i in range(reward_shaping_conf[\"time_window\"])]\n",
    "        rewards = np.float32(\n",
    "            reward_shaping_conf[\"fulfillment_factor\"] * np.dot(future_fulfillment.T, decay_list)\n",
    "            - reward_shaping_conf[\"shortage_factor\"] * np.dot(future_shortage.T, decay_list)\n",
    "        )\n",
    "        return {agent_id: reward for agent_id, reward in zip(ports, rewards)}\n",
    "\n",
    "    def _post_step(self, cache_element: CacheElement) -> None:\n",
    "        self._info[\"env_metric\"] = self._env.metrics\n",
    "\n",
    "    def _post_eval_step(self, cache_element: CacheElement) -> None:\n",
    "        self._post_step(cache_element)\n",
    "\n",
    "    def post_collect(self, info_list: list, ep: int) -> None:\n",
    "        # print the env metric from each rollout worker\n",
    "        for info in info_list:\n",
    "            print(f\"env summary (episode {ep}): {info['env_metric']}\")\n",
    "\n",
    "        # print the average env metric\n",
    "        if len(info_list) > 1:\n",
    "            metric_keys, num_envs = info_list[0][\"env_metric\"].keys(), len(info_list)\n",
    "            avg_metric = {key: sum(info[\"env_metric\"][key] for info in info_list) / num_envs for key in metric_keys}\n",
    "            print(f\"average env summary (episode {ep}): {avg_metric}\")\n",
    "\n",
    "    def post_evaluate(self, info_list: list, ep: int) -> None:\n",
    "        self.post_collect(info_list, ep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies & Trainers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = (\n",
    "    (state_shaping_conf[\"look_back\"] + 1) * (state_shaping_conf[\"max_ports_downstream\"] + 1) * len(port_attributes)\n",
    "    + len(vessel_attributes)\n",
    ")\n",
    "action_num = len(action_shaping_conf[\"action_space\"])\n",
    "\n",
    "actor_net_conf = {\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"activation\": torch.nn.Tanh,\n",
    "    \"softmax\": True,\n",
    "    \"batch_norm\": False,\n",
    "    \"head\": True,\n",
    "}\n",
    "critic_net_conf = {\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"output_dim\": 1,\n",
    "    \"activation\": torch.nn.LeakyReLU,\n",
    "    \"softmax\": False,\n",
    "    \"batch_norm\": True,\n",
    "    \"head\": True,\n",
    "}\n",
    "\n",
    "actor_learning_rate = 0.001\n",
    "critic_learning_rate = 0.001\n",
    "\n",
    "class MyActorNet(DiscreteACBasedNet):\n",
    "    def __init__(self, state_dim: int, action_num: int) -> None:\n",
    "        super(MyActorNet, self).__init__(state_dim=state_dim, action_num=action_num)\n",
    "        self._actor = FullyConnected(input_dim=state_dim, output_dim=action_num, **actor_net_conf)\n",
    "        self._optim = Adam(self._actor.parameters(), lr=actor_learning_rate)\n",
    "\n",
    "    def _get_action_probs_impl(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        return self._actor(states)\n",
    "\n",
    "\n",
    "class MyCriticNet(VNet):\n",
    "    def __init__(self, state_dim: int) -> None:\n",
    "        super(MyCriticNet, self).__init__(state_dim=state_dim)\n",
    "        self._critic = FullyConnected(input_dim=state_dim, **critic_net_conf)\n",
    "        self._optim = RMSprop(self._critic.parameters(), lr=critic_learning_rate)\n",
    "\n",
    "    def _get_v_values(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        return self._critic(states).squeeze(-1)\n",
    "\n",
    "def get_ppo_trainer(state_dim: int, name: str) -> PPOTrainer:\n",
    "    return PPOTrainer(\n",
    "        name=name,\n",
    "        params=PPOParams(\n",
    "            get_v_critic_net_func=lambda: MyCriticNet(state_dim),\n",
    "            reward_discount=.0,\n",
    "            grad_iters=10,\n",
    "            critic_loss_cls=torch.nn.SmoothL1Loss,\n",
    "            min_logp=None,\n",
    "            lam=.0,\n",
    "            clip_ratio=0.1,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "learn_env = Env(scenario=\"cim\", topology=\"toy.4p_ssdd_l0.0\", durations=500)\n",
    "test_env = learn_env\n",
    "num_agents = len(learn_env.agent_idx_list)\n",
    "agent2policy = {agent: f\"ppo_{agent}.policy\"for agent in learn_env.agent_idx_list}\n",
    "policies = [DiscretePolicyGradient(name=f\"ppo_{i}.policy\", policy_net=MyActorNet(state_dim, action_num)) for i in range(num_agents)]\n",
    "trainers = [get_ppo_trainer(state_dim, f\"ppo_{i}\") for i in range(num_agents)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL component bundle\n",
    "\n",
    "An RL component bundle integrate all necessary resources to launch a learning loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_component_bundle = RLComponentBundle(\n",
    "    env_sampler=CIMEnvSampler(\n",
    "        learn_env=learn_env,\n",
    "        test_env=test_env,\n",
    "        policies=policies,\n",
    "        agent2policy=agent2policy,\n",
    "        reward_eval_delay=reward_shaping_conf[\"time_window\"],\n",
    "    ),\n",
    "    agent2policy=agent2policy,\n",
    "    policies=policies,\n",
    "    trainers=trainers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Loop\n",
    "\n",
    "This code cell demonstrates a typical single-threaded training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting result:\n",
      "env summary (episode 1): {'order_requirements': 1000000, 'container_shortage': 632035, 'operation_number': 1572320}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 2): {'order_requirements': 1000000, 'container_shortage': 601747, 'operation_number': 1915139}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 3): {'order_requirements': 1000000, 'container_shortage': 574928, 'operation_number': 1804465}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 4): {'order_requirements': 1000000, 'container_shortage': 519569, 'operation_number': 1927023}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 5): {'order_requirements': 1000000, 'container_shortage': 516575, 'operation_number': 1582482}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 5): {'order_requirements': 1000000, 'container_shortage': 624892, 'operation_number': 716074}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 6): {'order_requirements': 1000000, 'container_shortage': 525403, 'operation_number': 1600461}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 7): {'order_requirements': 1000000, 'container_shortage': 399383, 'operation_number': 1843362}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 8): {'order_requirements': 1000000, 'container_shortage': 453590, 'operation_number': 1925248}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 9): {'order_requirements': 1000000, 'container_shortage': 353956, 'operation_number': 1717214}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 10): {'order_requirements': 1000000, 'container_shortage': 358955, 'operation_number': 1729121}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 10): {'order_requirements': 1000000, 'container_shortage': 645000, 'operation_number': 690814}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 11): {'order_requirements': 1000000, 'container_shortage': 444126, 'operation_number': 1615111}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 12): {'order_requirements': 1000000, 'container_shortage': 367739, 'operation_number': 1689650}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 13): {'order_requirements': 1000000, 'container_shortage': 332744, 'operation_number': 1645329}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 14): {'order_requirements': 1000000, 'container_shortage': 377685, 'operation_number': 1749329}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 15): {'order_requirements': 1000000, 'container_shortage': 307540, 'operation_number': 1671363}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 15): {'order_requirements': 1000000, 'container_shortage': 475847, 'operation_number': 1051874}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 16): {'order_requirements': 1000000, 'container_shortage': 317064, 'operation_number': 1835218}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 17): {'order_requirements': 1000000, 'container_shortage': 240318, 'operation_number': 1790099}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 18): {'order_requirements': 1000000, 'container_shortage': 354443, 'operation_number': 2027905}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 19): {'order_requirements': 1000000, 'container_shortage': 309117, 'operation_number': 1867812}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 20): {'order_requirements': 1000000, 'container_shortage': 261402, 'operation_number': 1871959}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 20): {'order_requirements': 1000000, 'container_shortage': 279266, 'operation_number': 1463833}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 21): {'order_requirements': 1000000, 'container_shortage': 224541, 'operation_number': 1791494}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 22): {'order_requirements': 1000000, 'container_shortage': 208655, 'operation_number': 1860491}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 23): {'order_requirements': 1000000, 'container_shortage': 215582, 'operation_number': 1925044}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 24): {'order_requirements': 1000000, 'container_shortage': 205048, 'operation_number': 1875270}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 25): {'order_requirements': 1000000, 'container_shortage': 243789, 'operation_number': 1856279}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 25): {'order_requirements': 1000000, 'container_shortage': 865687, 'operation_number': 207716}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 26): {'order_requirements': 1000000, 'container_shortage': 262272, 'operation_number': 1686045}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 27): {'order_requirements': 1000000, 'container_shortage': 202813, 'operation_number': 1837507}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 28): {'order_requirements': 1000000, 'container_shortage': 198031, 'operation_number': 1783175}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 29): {'order_requirements': 1000000, 'container_shortage': 242185, 'operation_number': 1728690}\n",
      "\n",
      "Collecting result:\n",
      "env summary (episode 30): {'order_requirements': 1000000, 'container_shortage': 160058, 'operation_number': 1944712}\n",
      "\n",
      "Evaluation result:\n",
      "env summary (episode 30): {'order_requirements': 1000000, 'container_shortage': 950000, 'operation_number': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env_sampler = rl_component_bundle.env_sampler\n",
    "\n",
    "num_episodes = 30\n",
    "eval_schedule = [5, 10, 15, 20, 25, 30]\n",
    "eval_point_index = 0\n",
    "\n",
    "training_manager = TrainingManager(rl_component_bundle=rl_component_bundle)\n",
    "\n",
    "# main loop\n",
    "for ep in range(1, num_episodes + 1):\n",
    "    result = env_sampler.sample()\n",
    "    experiences: List[List[ExpElement]] = result[\"experiences\"]\n",
    "    info_list: List[dict] = result[\"info\"]\n",
    "        \n",
    "    print(\"Collecting result:\")\n",
    "    env_sampler.post_collect(info_list, ep)\n",
    "    print()\n",
    "\n",
    "    training_manager.record_experiences(experiences)\n",
    "    training_manager.train_step()\n",
    "\n",
    "    if ep == eval_schedule[eval_point_index]:\n",
    "        eval_point_index += 1\n",
    "        result = env_sampler.eval()\n",
    "        \n",
    "        print(\"Evaluation result:\")\n",
    "        env_sampler.post_evaluate(result[\"info\"], ep)\n",
    "        print()\n",
    "\n",
    "training_manager.exit()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f57a09d39b50edfb56e79199ef40583334d721b06ead0e38a39e7e79092073c"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
