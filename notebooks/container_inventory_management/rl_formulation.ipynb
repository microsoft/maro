{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "This notebook demonstrates how to use MARO's reinforcement learning (RL) toolkit to solve the container inventory management ([CIM](https://maro.readthedocs.io/en/latest/scenarios/container_inventory_management.html)) problem. It is formalized as a multi-agent reinforcement learning problem, where each port acts as a decision agent. When a vessel arrives at a port, these agents must take actions by transfering a certain amount of containers to / from the vessel. The objective is for the agents to learn policies that minimize the cumulative container shortage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Common info\n",
    "common_config = {\n",
    "    \"port_attributes\": [\"empty\", \"full\", \"on_shipper\", \"on_consignee\", \"booking\", \"shortage\", \"fulfillment\"],\n",
    "    \"vessel_attributes\": [\"empty\", \"full\", \"remaining_space\"],\n",
    "    # Parameters for computing states\n",
    "    \"look_back\": 7,\n",
    "    \"max_ports_downstream\": 2,\n",
    "    # Parameters for computing actions\n",
    "    \"num_actions\": 21,\n",
    "    \"finite_vessel_space\": True,\n",
    "    \"has_early_discharge\": True,\n",
    "    # Parameters for computing rewards\n",
    "    \"reward_eval_delay\": 99,\n",
    "    \"fulfillment_factor\": 1.0,\n",
    "    \"shortage_factor\": 1.0,\n",
    "    \"time_decay\": 0.97\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maro.rl import AbsEnvWrapper\n",
    "from maro.simulator.scenarios.cim.common import Action, ActionType\n",
    "\n",
    "\n",
    "class CIMEnvWrapper(AbsEnvWrapper):\n",
    "    def __init__(\n",
    "        self, env, save_replay=True, replay_agent_ids=None, *, port_attributes, vessel_attributes, num_actions,\n",
    "        look_back,max_ports_downstream, reward_eval_delay, fulfillment_factor, shortage_factor, time_decay,\n",
    "        finite_vessel_space=True, has_early_discharge=True \n",
    "    ):\n",
    "        super().__init__(env, save_replay=save_replay, replay_agent_ids=replay_agent_ids, reward_eval_delay=reward_eval_delay)\n",
    "        self.port_attributes = port_attributes\n",
    "        self.vessel_attributes = vessel_attributes\n",
    "        self.action_space = list(np.linspace(-1.0, 1.0, num_actions))\n",
    "        self.look_back = look_back\n",
    "        self.max_ports_downstream = max_ports_downstream\n",
    "        self.fulfillment_factor = fulfillment_factor\n",
    "        self.shortage_factor = shortage_factor\n",
    "        self.time_decay = time_decay\n",
    "        self.finite_vessel_space = finite_vessel_space\n",
    "        self.has_early_discharge = has_early_discharge\n",
    "        self._last_action_tick = None\n",
    "\n",
    "    def get_state(self, tick=None):\n",
    "        if tick is None:\n",
    "            tick = self.env.tick\n",
    "        vessel_snapshots, port_snapshots = self.env.snapshot_list[\"vessels\"], self.env.snapshot_list[\"ports\"]\n",
    "        port_idx, vessel_idx = self.event.port_idx, self.event.vessel_idx\n",
    "        ticks = [max(0, tick - rt) for rt in range(self.look_back - 1)]\n",
    "        future_port_idx_list = vessel_snapshots[tick: vessel_idx: 'future_stop_list'].astype('int')\n",
    "        port_features = port_snapshots[ticks: [port_idx] + list(future_port_idx_list): self.port_attributes]\n",
    "        vessel_features = vessel_snapshots[tick: vessel_idx: self.vessel_attributes]\n",
    "        self.state_info = {\n",
    "            \"tick\": tick, \"action_scope\": self.event.action_scope, \"port_idx\": port_idx, \"vessel_idx\": vessel_idx\n",
    "        }\n",
    "        state = np.concatenate((port_features, vessel_features))\n",
    "        self._last_action_tick = tick\n",
    "        return {port_idx: state}\n",
    "\n",
    "    def to_env_action(self, action_by_agent):\n",
    "        vessel_snapshots = self.env.snapshot_list[\"vessels\"]\n",
    "        action_info = list(action_by_agent.values())[0]\n",
    "        model_action = action_info[0] if isinstance(action_info, tuple) else action_info\n",
    "        tick, port, vessel = self.state_info[\"tick\"], self.state_info[\"port_idx\"], self.state_info[\"vessel_idx\"]\n",
    "        zero_action_idx = len(self.action_space) / 2  # index corresponding to value zero.\n",
    "        vessel_space = vessel_snapshots[tick:vessel:self.vessel_attributes][2] if self.finite_vessel_space else float(\"inf\")\n",
    "        early_discharge = vessel_snapshots[tick:vessel:\"early_discharge\"][0] if self.has_early_discharge else 0\n",
    "        percent = abs(self.action_space[model_action])\n",
    "\n",
    "        action_scope = self.state_info[\"action_scope\"]\n",
    "        if model_action < zero_action_idx:\n",
    "            action_type = ActionType.LOAD\n",
    "            actual_action = min(round(percent * action_scope.load), vessel_space)\n",
    "        elif model_action > zero_action_idx:\n",
    "            action_type = ActionType.DISCHARGE\n",
    "            plan_action = percent * (action_scope.discharge + early_discharge) - early_discharge\n",
    "            actual_action = round(plan_action) if plan_action > 0 else round(percent * action_scope.discharge)\n",
    "        else:\n",
    "            actual_action, action_type = 0, None\n",
    "\n",
    "        return Action(vessel, port, actual_action, action_type)\n",
    "\n",
    "    def get_reward(self, tick=None):\n",
    "        \"\"\"Delayed reward evaluation.\"\"\"\n",
    "        if tick is None:\n",
    "            tick = self._last_action_tick\n",
    "        port_snapshots = self.env.snapshot_list[\"ports\"]\n",
    "        start_tick = tick + 1\n",
    "        ticks = list(range(start_tick, start_tick + self.reward_eval_delay))\n",
    "\n",
    "        future_fulfillment = port_snapshots[ticks::\"fulfillment\"]\n",
    "        future_shortage = port_snapshots[ticks::\"shortage\"]\n",
    "        decay_list = [\n",
    "            self.time_decay ** i for i in range(self.reward_eval_delay)\n",
    "            for _ in range(future_fulfillment.shape[0] // self.reward_eval_delay)\n",
    "        ]\n",
    "\n",
    "        return {\n",
    "            agent_id: np.float32(\n",
    "                self.fulfillment_factor * np.dot(future_fulfillment, decay_list) - \n",
    "                self.shortage_factor * np.dot(future_shortage, decay_list)\n",
    "            )\n",
    "            for agent_id in self.action_history[tick]\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Policy](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#agent)\n",
    "\n",
    "The out-of-the-box ActorCritic is used as our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from maro.rl import ActorCritic, ActorCriticConfig, DiscreteACNet, ExperienceManager, FullyConnectedBlock, OptimOption\n",
    "\n",
    "# We consider the port in question as well as two downstream ports.\n",
    "# We consider the states of these ports over the past 7 days plus the current day, hence the factor 8.\n",
    "input_dim = (\n",
    "    (common_config[\"look_back\"] + 1) *\n",
    "    (common_config[\"max_ports_downstream\"] + 1) *\n",
    "    len(common_config[\"port_attributes\"]) +\n",
    "    len(common_config[\"vessel_attributes\"])\n",
    ")\n",
    "\n",
    "policy_config = {\n",
    "    \"model\": { \n",
    "        \"network\": {\n",
    "            \"actor\": {\n",
    "                \"input_dim\": input_dim,\n",
    "                \"output_dim\": common_config[\"num_actions\"],\n",
    "                \"hidden_dims\": [256, 128, 64],\n",
    "                \"activation\": \"tanh\",\n",
    "                \"softmax\": True,\n",
    "                \"batch_norm\": False,\n",
    "                \"head\": True\n",
    "            },\n",
    "            \"critic\": {\n",
    "                \"input_dim\": input_dim,\n",
    "                \"output_dim\": 1,\n",
    "                \"hidden_dims\": [256, 128, 64],\n",
    "                \"activation\": \"leaky_relu\",\n",
    "                \"softmax\": False,\n",
    "                \"batch_norm\": True,\n",
    "                \"head\": True\n",
    "            }\n",
    "        },\n",
    "        \"optimization\": {\n",
    "            \"actor\": OptimOption(optim_cls=\"adam\", optim_params={\"lr\": 0.001}),\n",
    "            \"critic\": OptimOption(optim_cls=\"rmsprop\", optim_params={\"lr\": 0.001})\n",
    "        }\n",
    "    },\n",
    "    \"experience_manager\": {\n",
    "        \"capacity\": 10000\n",
    "    },\n",
    "    \"algorithm_config\": {\n",
    "        \"reward_discount\": .0,\n",
    "        \"train_epochs\": 10,\n",
    "        \"gradient_iters\": 1,\n",
    "        \"actor_loss_coefficient\": 0.1,  # loss = actor_loss_coefficient * actor_loss + critic_loss\n",
    "        \"critic_loss_cls\": \"smooth_l1\",\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class MyACNet(DiscreteACNet):\n",
    "    def forward(self, states, actor: bool = True, critic: bool = True):\n",
    "        states = torch.from_numpy(np.asarray(states))\n",
    "        if len(states.shape) == 1:\n",
    "            states = states.unsqueeze(dim=0)\n",
    "\n",
    "        states = states.to(self.device)\n",
    "        return (\n",
    "            self.component[\"actor\"](states) if actor else None,\n",
    "            self.component[\"critic\"](states) if critic else None\n",
    "        )\n",
    "\n",
    "\n",
    "def get_ac_policy(name):\n",
    "    actor = FullyConnectedBlock(**policy_config[\"model\"][\"network\"][\"actor\"])\n",
    "    critic = FullyConnectedBlock(**policy_config[\"model\"][\"network\"][\"critic\"])\n",
    "    ac_net = MyACNet({\"actor\": actor, \"critic\": critic}, optim_option=policy_config[\"model\"][\"optimization\"])\n",
    "    experience_manager = ExperienceManager(policy_config[\"experience_manager\"][\"capacity\"])\n",
    "    return ActorCritic(name, ac_net, experience_manager, ActorCriticConfig(**policy_config[\"algorithm_config\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This code cell demonstrates a typical single-threaded training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.simulator import Env\n",
    "from maro.rl import SimpleLearner\n",
    "from maro.utils import set_seeds\n",
    "\n",
    "set_seeds(1024)  # for reproducibility\n",
    "env = Env(\"cim\", \"toy.4p_ssdd_l0.0\", durations=1120)\n",
    "env_wrapper = CIMEnvWrapper(env, **common_config)\n",
    "policies = [get_ac_policy(id_) for id_ in env.agent_idx_list]\n",
    "agent2policy = {agent_id: agent_id for agent_id in env.agent_idx_list}\n",
    "learner = SimpleLearner(env_wrapper, policies, agent2policy, 40)  # 40 episodes\n",
    "learner.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maro",
   "language": "python",
   "name": "maro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
