{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "This notebook demonstrates how to use MARO's reinforcement learning (RL) toolkit to solve the container inventory management ([CIM](https://maro.readthedocs.io/en/latest/scenarios/container_inventory_management.html)) problem. It is formalized as a multi-agent reinforcement learning problem, where each port acts as a decision agent. When a vessel arrives at a port, these agents must take actions by transfering a certain amount of containers to / from the vessel. The objective is for the agents to learn policies that minimize the cumulative container shortage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Common info\n",
    "common_config = {\n",
    "    \"port_attributes\": [\"empty\", \"full\", \"on_shipper\", \"on_consignee\", \"booking\", \"shortage\", \"fulfillment\"],\n",
    "    \"vessel_attributes\": [\"empty\", \"full\", \"remaining_space\"],\n",
    "    \"action_space\": list(np.linspace(-1.0, 1.0, 21)),\n",
    "    # Parameters for computing states\n",
    "    \"look_back\": 7,\n",
    "    \"max_ports_downstream\": 2,\n",
    "    # Parameters for computing actions\n",
    "    \"finite_vessel_space\": True,\n",
    "    \"has_early_discharge\": True,\n",
    "    # Parameters for computing rewards\n",
    "    \"reward_time_window\": 99,\n",
    "    \"fulfillment_factor\": 1.0,\n",
    "    \"shortage_factor\": 1.0,\n",
    "    \"time_decay\": 0.97\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from maro.rl import Trajectory\n",
    "from maro.simulator.scenarios.cim.common import Action, ActionType\n",
    "\n",
    "\n",
    "class CIMTrajectory(Trajectory):\n",
    "    def __init__(\n",
    "        self, env, *, port_attributes, vessel_attributes, action_space, look_back, max_ports_downstream,\n",
    "        reward_time_window, fulfillment_factor, shortage_factor, time_decay,\n",
    "        finite_vessel_space=True, has_early_discharge=True \n",
    "    ):\n",
    "        super().__init__(env)\n",
    "        self.port_attributes = port_attributes\n",
    "        self.vessel_attributes = vessel_attributes\n",
    "        self.action_space = action_space\n",
    "        self.look_back = look_back\n",
    "        self.max_ports_downstream = max_ports_downstream\n",
    "        self.reward_time_window = reward_time_window\n",
    "        self.fulfillment_factor = fulfillment_factor\n",
    "        self.shortage_factor = shortage_factor\n",
    "        self.time_decay = time_decay\n",
    "        self.finite_vessel_space = finite_vessel_space\n",
    "        self.has_early_discharge = has_early_discharge\n",
    "\n",
    "    def get_state(self, event):\n",
    "        vessel_snapshots, port_snapshots = self.env.snapshot_list[\"vessels\"], self.env.snapshot_list[\"ports\"]\n",
    "        tick, port_idx, vessel_idx = event.tick, event.port_idx, event.vessel_idx\n",
    "        ticks = [max(0, tick - rt) for rt in range(self.look_back - 1)]\n",
    "        future_port_idx_list = vessel_snapshots[tick: vessel_idx: 'future_stop_list'].astype('int')\n",
    "        port_features = port_snapshots[ticks: [port_idx] + list(future_port_idx_list): self.port_attributes]\n",
    "        vessel_features = vessel_snapshots[tick: vessel_idx: self.vessel_attributes]\n",
    "        return {port_idx: np.concatenate((port_features, vessel_features))}\n",
    "\n",
    "    def get_action(self, action_by_agent, event):\n",
    "        vessel_snapshots = self.env.snapshot_list[\"vessels\"]\n",
    "        action_info = list(action_by_agent.values())[0]\n",
    "        model_action = action_info[0] if isinstance(action_info, tuple) else action_info\n",
    "        scope, tick, port, vessel = event.action_scope, event.tick, event.port_idx, event.vessel_idx\n",
    "        zero_action_idx = len(self.action_space) / 2  # index corresponding to value zero.\n",
    "        vessel_space = vessel_snapshots[tick:vessel:self.vessel_attributes][2] if self.finite_vessel_space else float(\"inf\")\n",
    "        early_discharge = vessel_snapshots[tick:vessel:\"early_discharge\"][0] if self.has_early_discharge else 0\n",
    "        percent = abs(self.action_space[model_action])\n",
    "\n",
    "        if model_action < zero_action_idx:\n",
    "            action_type = ActionType.LOAD\n",
    "            actual_action = min(round(percent * scope.load), vessel_space)\n",
    "        elif model_action > zero_action_idx:\n",
    "            action_type = ActionType.DISCHARGE\n",
    "            plan_action = percent * (scope.discharge + early_discharge) - early_discharge\n",
    "            actual_action = round(plan_action) if plan_action > 0 else round(percent * scope.discharge)\n",
    "        else:\n",
    "            actual_action, action_type = 0, None\n",
    "\n",
    "        return {port: Action(vessel, port, actual_action, action_type)}\n",
    "\n",
    "    def get_offline_reward(self, event):\n",
    "        port_snapshots = self.env.snapshot_list[\"ports\"]\n",
    "        start_tick = event.tick + 1\n",
    "        ticks = list(range(start_tick, start_tick + self.reward_time_window))\n",
    "\n",
    "        future_fulfillment = port_snapshots[ticks::\"fulfillment\"]\n",
    "        future_shortage = port_snapshots[ticks::\"shortage\"]\n",
    "        decay_list = [\n",
    "            self.time_decay ** i for i in range(self.reward_time_window)\n",
    "            for _ in range(future_fulfillment.shape[0] // self.reward_time_window)\n",
    "        ]\n",
    "\n",
    "        tot_fulfillment = np.dot(future_fulfillment, decay_list)\n",
    "        tot_shortage = np.dot(future_shortage, decay_list)\n",
    "\n",
    "        return np.float32(self.fulfillment_factor * tot_fulfillment - self.shortage_factor * tot_shortage)\n",
    "\n",
    "    def on_env_feedback(self, event, state_by_agent, action_by_agent, reward):\n",
    "        self.trajectory[\"event\"].append(event)\n",
    "        self.trajectory[\"state\"].append(state_by_agent)\n",
    "        self.trajectory[\"action\"].append(action_by_agent)\n",
    "    \n",
    "    def on_finish(self):\n",
    "        training_data = {}\n",
    "        for event, state, action in zip(self.trajectory[\"event\"], self.trajectory[\"state\"], self.trajectory[\"action\"]):\n",
    "            agent_id = list(state.keys())[0]\n",
    "            data = training_data.setdefault(agent_id, {\"args\": [[] for _ in range(4)]})\n",
    "            data[\"args\"][0].append(state[agent_id])  # state\n",
    "            data[\"args\"][1].append(action[agent_id][0])  # action\n",
    "            data[\"args\"][2].append(action[agent_id][1])  # log_p\n",
    "            data[\"args\"][3].append(self.get_offline_reward(event))  # reward\n",
    "\n",
    "        for agent_id in training_data:\n",
    "            training_data[agent_id][\"args\"] = [\n",
    "                np.asarray(vals, dtype=np.float32 if i == 3 else None)\n",
    "                for i, vals in enumerate(training_data[agent_id][\"args\"])\n",
    "            ]\n",
    "\n",
    "        return training_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Agent](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#agent)\n",
    "\n",
    "The out-of-the-box ActorCritic is used as our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import Adam, RMSprop\n",
    "\n",
    "from maro.rl import ActorCritic, ActorCriticConfig, FullyConnectedBlock, OptimOption, SimpleMultiHeadModel\n",
    "\n",
    "# We consider the port in question as well as two downstream ports.\n",
    "# We consider the states of these ports over the past 7 days plus the current day, hence the factor 8.\n",
    "input_dim = (\n",
    "    (common_config[\"look_back\"] + 1) *\n",
    "    (common_config[\"max_ports_downstream\"] + 1) *\n",
    "    len(common_config[\"port_attributes\"]) +\n",
    "    len(common_config[\"vessel_attributes\"])\n",
    ")\n",
    "\n",
    "agent_config = {\n",
    "    \"model\": {\n",
    "        \"actor\": {\n",
    "            \"input_dim\": input_dim,\n",
    "            \"output_dim\": len(common_config[\"action_space\"]),\n",
    "            \"hidden_dims\": [256, 128, 64],\n",
    "            \"activation\": nn.Tanh,\n",
    "            \"softmax\": True,\n",
    "            \"batch_norm\": False,\n",
    "            \"head\": True\n",
    "        },\n",
    "        \"critic\": {\n",
    "            \"input_dim\": input_dim,\n",
    "            \"output_dim\": 1,\n",
    "            \"hidden_dims\": [256, 128, 64],\n",
    "            \"activation\": nn.LeakyReLU,\n",
    "            \"softmax\": False,\n",
    "            \"batch_norm\": True,\n",
    "            \"head\": True\n",
    "        }\n",
    "    },\n",
    "    \"optimization\": {\n",
    "        \"actor\": OptimOption(optim_cls=Adam, optim_params={\"lr\": 0.001}),\n",
    "        \"critic\": OptimOption(optim_cls=RMSprop, optim_params={\"lr\": 0.001})\n",
    "    },\n",
    "    \"hyper_params\": {\n",
    "        \"reward_discount\": .0,\n",
    "        \"critic_loss_func\": nn.SmoothL1Loss(),\n",
    "        \"train_iters\": 10,\n",
    "        \"actor_loss_coefficient\": 0.1,  # loss = actor_loss_coefficient * actor_loss + critic_loss\n",
    "        \"k\": 1,  # for k-step return\n",
    "        \"lam\": 0.0  # lambda return coefficient\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_ac_agent():\n",
    "    actor_net = FullyConnectedBlock(**agent_config[\"model\"][\"actor\"])\n",
    "    critic_net = FullyConnectedBlock(**agent_config[\"model\"][\"critic\"])\n",
    "    ac_model = SimpleMultiHeadModel(\n",
    "        {\"actor\": actor_net, \"critic\": critic_net}, optim_option=agent_config[\"optimization\"],\n",
    "    )\n",
    "    return ActorCritic(ac_model, ActorCriticConfig(**agent_config[\"hyper_params\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This code cell demonstrates a typical single-threaded training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:54:17 | LEARNER | INFO | ep-0: {'order_requirements': 2240000, 'container_shortage': 1422736, 'operation_number': 4220466}\n",
      "14:54:19 | LEARNER | INFO | Agent learning finished\n",
      "14:54:23 | LEARNER | INFO | ep-1: {'order_requirements': 2240000, 'container_shortage': 1330641, 'operation_number': 3919970}\n",
      "14:54:24 | LEARNER | INFO | Agent learning finished\n",
      "14:54:29 | LEARNER | INFO | ep-2: {'order_requirements': 2240000, 'container_shortage': 996878, 'operation_number': 3226186}\n",
      "14:54:30 | LEARNER | INFO | Agent learning finished\n",
      "14:54:34 | LEARNER | INFO | ep-3: {'order_requirements': 2240000, 'container_shortage': 703662, 'operation_number': 3608511}\n",
      "14:54:36 | LEARNER | INFO | Agent learning finished\n",
      "14:54:40 | LEARNER | INFO | ep-4: {'order_requirements': 2240000, 'container_shortage': 601934, 'operation_number': 3579281}\n",
      "14:54:41 | LEARNER | INFO | Agent learning finished\n",
      "14:54:45 | LEARNER | INFO | ep-5: {'order_requirements': 2240000, 'container_shortage': 629344, 'operation_number': 3456707}\n",
      "14:54:47 | LEARNER | INFO | Agent learning finished\n",
      "14:54:51 | LEARNER | INFO | ep-6: {'order_requirements': 2240000, 'container_shortage': 560709, 'operation_number': 3511869}\n",
      "14:54:52 | LEARNER | INFO | Agent learning finished\n",
      "14:54:56 | LEARNER | INFO | ep-7: {'order_requirements': 2240000, 'container_shortage': 483549, 'operation_number': 3613713}\n",
      "14:54:57 | LEARNER | INFO | Agent learning finished\n",
      "14:55:02 | LEARNER | INFO | ep-8: {'order_requirements': 2240000, 'container_shortage': 390332, 'operation_number': 3817820}\n",
      "14:55:03 | LEARNER | INFO | Agent learning finished\n",
      "14:55:07 | LEARNER | INFO | ep-9: {'order_requirements': 2240000, 'container_shortage': 361151, 'operation_number': 3823994}\n",
      "14:55:08 | LEARNER | INFO | Agent learning finished\n",
      "14:55:13 | LEARNER | INFO | ep-10: {'order_requirements': 2240000, 'container_shortage': 442086, 'operation_number': 3647343}\n",
      "14:55:14 | LEARNER | INFO | Agent learning finished\n",
      "14:55:18 | LEARNER | INFO | ep-11: {'order_requirements': 2240000, 'container_shortage': 390846, 'operation_number': 3784078}\n",
      "14:55:19 | LEARNER | INFO | Agent learning finished\n",
      "14:55:24 | LEARNER | INFO | ep-12: {'order_requirements': 2240000, 'container_shortage': 309105, 'operation_number': 3896184}\n",
      "14:55:25 | LEARNER | INFO | Agent learning finished\n",
      "14:55:29 | LEARNER | INFO | ep-13: {'order_requirements': 2240000, 'container_shortage': 430801, 'operation_number': 3787247}\n",
      "14:55:30 | LEARNER | INFO | Agent learning finished\n",
      "14:55:35 | LEARNER | INFO | ep-14: {'order_requirements': 2240000, 'container_shortage': 368042, 'operation_number': 3793428}\n",
      "14:55:36 | LEARNER | INFO | Agent learning finished\n",
      "14:55:40 | LEARNER | INFO | ep-15: {'order_requirements': 2240000, 'container_shortage': 383015, 'operation_number': 3829184}\n",
      "14:55:41 | LEARNER | INFO | Agent learning finished\n",
      "14:55:46 | LEARNER | INFO | ep-16: {'order_requirements': 2240000, 'container_shortage': 373584, 'operation_number': 3772635}\n",
      "14:55:47 | LEARNER | INFO | Agent learning finished\n",
      "14:55:51 | LEARNER | INFO | ep-17: {'order_requirements': 2240000, 'container_shortage': 411397, 'operation_number': 3644350}\n",
      "14:55:53 | LEARNER | INFO | Agent learning finished\n",
      "14:55:57 | LEARNER | INFO | ep-18: {'order_requirements': 2240000, 'container_shortage': 307861, 'operation_number': 3842550}\n",
      "14:55:58 | LEARNER | INFO | Agent learning finished\n",
      "14:56:02 | LEARNER | INFO | ep-19: {'order_requirements': 2240000, 'container_shortage': 324650, 'operation_number': 3848202}\n",
      "14:56:04 | LEARNER | INFO | Agent learning finished\n",
      "14:56:08 | LEARNER | INFO | ep-20: {'order_requirements': 2240000, 'container_shortage': 367267, 'operation_number': 3739414}\n",
      "14:56:09 | LEARNER | INFO | Agent learning finished\n",
      "14:56:13 | LEARNER | INFO | ep-21: {'order_requirements': 2240000, 'container_shortage': 326153, 'operation_number': 3822407}\n",
      "14:56:15 | LEARNER | INFO | Agent learning finished\n",
      "14:56:19 | LEARNER | INFO | ep-22: {'order_requirements': 2240000, 'container_shortage': 466237, 'operation_number': 3516845}\n",
      "14:56:20 | LEARNER | INFO | Agent learning finished\n",
      "14:56:25 | LEARNER | INFO | ep-23: {'order_requirements': 2240000, 'container_shortage': 429538, 'operation_number': 3603386}\n",
      "14:56:26 | LEARNER | INFO | Agent learning finished\n",
      "14:56:30 | LEARNER | INFO | ep-24: {'order_requirements': 2240000, 'container_shortage': 241307, 'operation_number': 3986364}\n",
      "14:56:31 | LEARNER | INFO | Agent learning finished\n",
      "14:56:36 | LEARNER | INFO | ep-25: {'order_requirements': 2240000, 'container_shortage': 260224, 'operation_number': 3971519}\n",
      "14:56:37 | LEARNER | INFO | Agent learning finished\n",
      "14:56:41 | LEARNER | INFO | ep-26: {'order_requirements': 2240000, 'container_shortage': 190507, 'operation_number': 4060439}\n",
      "14:56:42 | LEARNER | INFO | Agent learning finished\n",
      "14:56:47 | LEARNER | INFO | ep-27: {'order_requirements': 2240000, 'container_shortage': 152822, 'operation_number': 4146195}\n",
      "14:56:48 | LEARNER | INFO | Agent learning finished\n",
      "14:56:52 | LEARNER | INFO | ep-28: {'order_requirements': 2240000, 'container_shortage': 91878, 'operation_number': 4300404}\n",
      "14:56:53 | LEARNER | INFO | Agent learning finished\n",
      "14:56:58 | LEARNER | INFO | ep-29: {'order_requirements': 2240000, 'container_shortage': 78752, 'operation_number': 4297044}\n",
      "14:56:59 | LEARNER | INFO | Agent learning finished\n",
      "14:57:03 | LEARNER | INFO | ep-30: {'order_requirements': 2240000, 'container_shortage': 202098, 'operation_number': 4047921}\n",
      "14:57:04 | LEARNER | INFO | Agent learning finished\n",
      "14:57:09 | LEARNER | INFO | ep-31: {'order_requirements': 2240000, 'container_shortage': 161871, 'operation_number': 4113281}\n",
      "14:57:10 | LEARNER | INFO | Agent learning finished\n",
      "14:57:14 | LEARNER | INFO | ep-32: {'order_requirements': 2240000, 'container_shortage': 74649, 'operation_number': 4311775}\n",
      "14:57:16 | LEARNER | INFO | Agent learning finished\n",
      "14:57:20 | LEARNER | INFO | ep-33: {'order_requirements': 2240000, 'container_shortage': 54402, 'operation_number': 4330703}\n",
      "14:57:21 | LEARNER | INFO | Agent learning finished\n",
      "14:57:26 | LEARNER | INFO | ep-34: {'order_requirements': 2240000, 'container_shortage': 42802, 'operation_number': 4353353}\n",
      "14:57:27 | LEARNER | INFO | Agent learning finished\n",
      "14:57:31 | LEARNER | INFO | ep-35: {'order_requirements': 2240000, 'container_shortage': 49236, 'operation_number': 4346898}\n",
      "14:57:32 | LEARNER | INFO | Agent learning finished\n",
      "14:57:37 | LEARNER | INFO | ep-36: {'order_requirements': 2240000, 'container_shortage': 74055, 'operation_number': 4280054}\n",
      "14:57:38 | LEARNER | INFO | Agent learning finished\n",
      "14:57:42 | LEARNER | INFO | ep-37: {'order_requirements': 2240000, 'container_shortage': 66899, 'operation_number': 4312042}\n",
      "14:57:43 | LEARNER | INFO | Agent learning finished\n",
      "14:57:48 | LEARNER | INFO | ep-38: {'order_requirements': 2240000, 'container_shortage': 29641, 'operation_number': 4385481}\n",
      "14:57:49 | LEARNER | INFO | Agent learning finished\n",
      "14:57:53 | LEARNER | INFO | ep-39: {'order_requirements': 2240000, 'container_shortage': 56018, 'operation_number': 4354815}\n",
      "14:57:54 | LEARNER | INFO | Agent learning finished\n"
     ]
    }
   ],
   "source": [
    "from maro.simulator import Env\n",
    "from maro.rl import Actor, MultiAgentWrapper, OnPolicyLearner\n",
    "from maro.utils import set_seeds\n",
    "\n",
    "set_seeds(1024)  # for reproducibility\n",
    "env = Env(\"cim\", \"toy.4p_ssdd_l0.0\", durations=1120)\n",
    "agent = MultiAgentWrapper({name: get_ac_agent() for name in env.agent_idx_list})\n",
    "actor = Actor(env, agent, CIMTrajectory, trajectory_kwargs=common_config)\n",
    "learner = OnPolicyLearner(actor, 40)  # 40 episodes\n",
    "learner.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maro",
   "language": "python",
   "name": "maro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
