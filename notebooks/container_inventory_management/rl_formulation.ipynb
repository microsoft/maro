{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "This notebook demonstrates how to use MARO's reinforcement learning (RL) toolkit to solve the container inventory management ([CIM](https://maro.readthedocs.io/en/latest/scenarios/container_inventory_management.html)) problem. It is formalized as a multi-agent reinforcement learning problem, where each port acts as a decision agent. When a vessel arrives at a port, these agents must take actions by transfering a certain amount of containers to / from the vessel. The objective is for the agents to learn policies that minimize the cumulative container shortage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env and shaping config\n",
    "env_conf = {\n",
    "    \"scenario\": \"cim\",\n",
    "    \"topology\": \"toy.4p_ssdd_l0.0\",\n",
    "    \"durations\": 560\n",
    "}\n",
    "\n",
    "port_attributes = [\"empty\", \"full\", \"on_shipper\", \"on_consignee\", \"booking\", \"shortage\", \"fulfillment\"]\n",
    "vessel_attributes = [\"empty\", \"full\", \"remaining_space\"]\n",
    "\n",
    "state_shaping_conf = {\n",
    "    \"look_back\": 7,\n",
    "    \"max_ports_downstream\": 2\n",
    "}\n",
    "\n",
    "action_shaping_conf = {\n",
    "    \"action_space\": [(i - 10) / 10 for i in range(21)],\n",
    "    \"finite_vessel_space\": True,\n",
    "    \"has_early_discharge\": True\n",
    "}\n",
    "\n",
    "reward_shaping_conf = {\n",
    "    \"time_window\": 99,\n",
    "    \"fulfillment_factor\": 1.0,\n",
    "    \"shortage_factor\": 1.0,\n",
    "    \"time_decay\": 0.97\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maro.rl.learning import AbsEnvSampler\n",
    "from maro.simulator.scenarios.cim.common import Action, ActionType\n",
    "\n",
    "\n",
    "class CIMEnvSampler(AbsEnvSampler):\n",
    "    def get_state(self, tick=None):\n",
    "        if tick is None:\n",
    "            tick = self.env.tick\n",
    "        vessel_snapshots, port_snapshots = self.env.snapshot_list[\"vessels\"], self.env.snapshot_list[\"ports\"]\n",
    "        port_idx, vessel_idx = self.event.port_idx, self.event.vessel_idx\n",
    "        ticks = [max(0, tick - rt) for rt in range(state_shaping_conf[\"look_back\"] - 1)]\n",
    "        future_port_list = vessel_snapshots[tick: vessel_idx: 'future_stop_list'].astype('int') \n",
    "        state = np.concatenate([\n",
    "            port_snapshots[ticks : [port_idx] + list(future_port_list) : port_attributes],\n",
    "            vessel_snapshots[tick : vessel_idx : vessel_attributes]\n",
    "        ])\n",
    "        return {port_idx: state}\n",
    "\n",
    "    def get_env_actions(self, action_by_agent):\n",
    "        action_space = action_shaping_conf[\"action_space\"]\n",
    "        finite_vsl_space = action_shaping_conf[\"finite_vessel_space\"]\n",
    "        has_early_discharge = action_shaping_conf[\"has_early_discharge\"]\n",
    "\n",
    "        port_idx, action = list(action_by_agent.items()).pop()\n",
    "        vsl_idx, action_scope = self.event.vessel_idx, self.event.action_scope\n",
    "        vsl_snapshots = self.env.snapshot_list[\"vessels\"]\n",
    "        vsl_space = vsl_snapshots[self.env.tick:vsl_idx:vessel_attributes][2] if finite_vsl_space else float(\"inf\")\n",
    "\n",
    "        model_action = action[\"action\"] if isinstance(action, dict) else action    \n",
    "        percent = abs(action_space[model_action])\n",
    "        zero_action_idx = len(action_space) / 2  # index corresponding to value zero.\n",
    "        if model_action < zero_action_idx:\n",
    "            action_type = ActionType.LOAD\n",
    "            actual_action = min(round(percent * action_scope.load), vsl_space)\n",
    "        elif model_action > zero_action_idx:\n",
    "            action_type = ActionType.DISCHARGE\n",
    "            early_discharge = vsl_snapshots[self.env.tick:vsl_idx:\"early_discharge\"][0] if has_early_discharge else 0\n",
    "            plan_action = percent * (action_scope.discharge + early_discharge) - early_discharge\n",
    "            actual_action = round(plan_action) if plan_action > 0 else round(percent * action_scope.discharge)\n",
    "        else:\n",
    "            actual_action, action_type = 0, None\n",
    "\n",
    "        return [Action(port_idx=port_idx, vessel_idx=vsl_idx, quantity=actual_action, action_type=action_type)]\n",
    "\n",
    "    def get_reward(self, actions, tick):\n",
    "        \"\"\"Delayed reward evaluation.\"\"\"\n",
    "        start_tick = tick + 1\n",
    "        ticks = list(range(start_tick, start_tick + reward_shaping_conf[\"time_window\"]))\n",
    "\n",
    "        # Get the ports that took actions at the given tick\n",
    "        ports = [action.port_idx for action in actions]\n",
    "        port_snapshots = self.env.snapshot_list[\"ports\"]\n",
    "        future_fulfillment = port_snapshots[ticks:ports:\"fulfillment\"].reshape(len(ticks), -1)\n",
    "        future_shortage = port_snapshots[ticks:ports:\"shortage\"].reshape(len(ticks), -1)\n",
    "\n",
    "        decay_list = [reward_shaping_conf[\"time_decay\"] ** i for i in range(reward_shaping_conf[\"time_window\"])]\n",
    "        rewards = np.float32(\n",
    "            reward_shaping_conf[\"fulfillment_factor\"] * np.dot(future_fulfillment.T, decay_list)\n",
    "            - reward_shaping_conf[\"shortage_factor\"] * np.dot(future_shortage.T, decay_list)\n",
    "        )\n",
    "        return {agent_id: reward for agent_id, reward in zip(ports, rewards)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Policies](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#agent)\n",
    "\n",
    "The out-of-the-box ActorCritic is used as our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam, RMSprop\n",
    "\n",
    "from maro.rl.exploration import MultiLinearExplorationScheduler, epsilon_greedy\n",
    "from maro.rl.modeling import DiscreteACNet, FullyConnected\n",
    "from maro.rl.policy import ActorCritic\n",
    "\n",
    "# We consider the port in question as well as two downstream ports.\n",
    "# We consider the states of these ports over the past 7 days plus the current day, hence the factor 8.\n",
    "# obtain state dimension from a temporary env_wrapper instance\n",
    "state_dim = (\n",
    "    (state_shaping_conf[\"look_back\"] + 1) * (state_shaping_conf[\"max_ports_downstream\"] + 1) * len(port_attributes)\n",
    "    + len(vessel_attributes)\n",
    ")\n",
    "\n",
    "# AC settings\n",
    "actor_net_conf = {\n",
    "    \"input_dim\": state_dim,\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"output_dim\": len(action_shaping_conf[\"action_space\"]),\n",
    "    \"activation\": torch.nn.Tanh,\n",
    "    \"softmax\": True,\n",
    "    \"batch_norm\": False,\n",
    "    \"head\": True\n",
    "}\n",
    "\n",
    "critic_net_conf = {\n",
    "    \"input_dim\": state_dim,\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"output_dim\": 1,\n",
    "    \"activation\": torch.nn.LeakyReLU,\n",
    "    \"softmax\": False,\n",
    "    \"batch_norm\": True,\n",
    "    \"head\": True\n",
    "}\n",
    "\n",
    "actor_optim_conf = (Adam, {\"lr\": 0.001})\n",
    "critic_optim_conf = (RMSprop, {\"lr\": 0.001})\n",
    "\n",
    "ac_conf = {\n",
    "    \"reward_discount\": .0,\n",
    "    \"grad_iters\": 10,\n",
    "    \"critic_loss_cls\": torch.nn.SmoothL1Loss,\n",
    "    \"min_logp\": None,\n",
    "    \"critic_loss_coeff\": 0.1,\n",
    "    \"entropy_coeff\": 0.01,\n",
    "    # \"clip_ratio\": 0.8   # for PPO\n",
    "    \"lam\": .0,\n",
    "    \"get_loss_on_rollout\": False\n",
    "}\n",
    "\n",
    "\n",
    "class MyACNet(DiscreteACNet):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.actor = FullyConnected(**actor_net_conf)\n",
    "        self.critic = FullyConnected(**critic_net_conf)\n",
    "        self.actor_optim = actor_optim_conf[0](self.actor.parameters(), **actor_optim_conf[1])\n",
    "        self.critic_optim = critic_optim_conf[0](self.critic.parameters(), **critic_optim_conf[1])\n",
    "\n",
    "    @property\n",
    "    def input_dim(self):\n",
    "        return state_dim\n",
    "\n",
    "    @property\n",
    "    def num_actions(self):\n",
    "        return q_net_conf[\"output_dim\"]\n",
    "\n",
    "    def forward(self, states, actor: bool = True, critic: bool = True):\n",
    "        return (self.actor(states) if actor else None), (self.critic(states) if critic else None)\n",
    "\n",
    "    def step(self, loss):\n",
    "        self.actor_optim.zero_grad()\n",
    "        self.critic_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.actor_optim.step()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "    def get_gradients(self, loss):\n",
    "        self.actor_optim.zero_grad()\n",
    "        self.critic_optim.zero_grad()\n",
    "        loss.backward()\n",
    "        return {name: param.grad for name, param in self.named_parameters()}\n",
    "\n",
    "    def apply_gradients(self, grad):\n",
    "        for name, param in self.named_parameters():\n",
    "            param.grad = grad[name]\n",
    "\n",
    "        self.actor_optim.step()\n",
    "        self.critic_optim.step()\n",
    "\n",
    "\n",
    "policy_func_dict = {f\"ac.{i}\": lambda name: ActorCritic(name, MyACNet(), **ac_conf) for i in range(4)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "This code cell demonstrates a typical single-threaded training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from maro.simulator import Env\n",
    "from maro.rl.learning import simple_learner\n",
    "from maro.utils import set_seeds\n",
    "\n",
    "# post-env-step callback\n",
    "def post_step(env, tracker, state, action, env_action, reward, tick):\n",
    "    tracker[\"env_metric\"] = env.metrics\n",
    "\n",
    "def get_env_sampler():\n",
    "    return CIMEnvSampler(\n",
    "        get_env=lambda: Env(**env_conf),\n",
    "        get_policy_func_dict=policy_func_dict,\n",
    "        agent2policy={agent: f\"ac.{agent}\" for agent in Env(**env_conf).agent_idx_list},\n",
    "        reward_eval_delay=reward_shaping_conf[\"time_window\"],\n",
    "        post_step=post_step\n",
    "    )\n",
    "\n",
    "# post-episode callback\n",
    "def post_collect(trackers, ep, segment):\n",
    "    # print the env metric from each rollout worker\n",
    "    for tracker in trackers:\n",
    "        print(f\"env summary (episode {ep}, segment {segment}): {tracker['env_metric']}\")\n",
    "\n",
    "    # print the average env metric\n",
    "    if len(trackers) > 1:\n",
    "        metric_keys, num_trackers = trackers[0][\"env_metric\"].keys(), len(trackers)\n",
    "        avg_metric = {key: sum(tr[\"env_metric\"][key] for tr in trackers) / num_trackers for key in metric_keys}\n",
    "        print(f\"average env summary (episode {ep}, segment {segment}): {avg_metric}\")\n",
    "\n",
    "# post-evaluation callback\n",
    "def post_evaluate(trackers, ep):\n",
    "    # print the env metric from each rollout worker\n",
    "    for tracker in trackers:\n",
    "        print(f\"env summary (evaluation episode {ep}): {tracker['env_metric']}\")\n",
    "\n",
    "    # print the average env metric\n",
    "    if len(trackers) > 1:\n",
    "        metric_keys, num_trackers = trackers[0][\"env_metric\"].keys(), len(trackers)\n",
    "        avg_metric = {key: sum(tr[\"env_metric\"][key] for tr in trackers) / num_trackers for key in metric_keys}\n",
    "        simulation_logger.info(f\"average env summary (evaluation episode {ep}): {avg_metric}\")\n",
    "\n",
    "\n",
    "set_seeds(1024)  # for reproducibility\n",
    "simple_learner(get_env_sampler, num_episodes=50, post_collect=post_collect, post_evaluate=post_evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "maro",
   "language": "python",
   "name": "maro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
