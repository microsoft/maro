{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "This notebook demonstrates the use of MARO's RL toolkit to optimize container inventory management. The scenario consists of a set of ports, each acting as a learning agent, and vessels that transfer empty containers among them. Each port must decide 1) whether to load or discharge containers when a vessel arrives and 2) how many containers to be loaded or discharged. The objective is to minimize the overall container shortage over a certain period of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env and shaping config\n",
    "env_conf = {\n",
    "    \"scenario\": \"cim\",\n",
    "    \"topology\": \"toy.4p_ssdd_l0.0\",\n",
    "    \"durations\": 560\n",
    "}\n",
    "\n",
    "port_attributes = [\"empty\", \"full\", \"on_shipper\", \"on_consignee\", \"booking\", \"shortage\", \"fulfillment\"]\n",
    "vessel_attributes = [\"empty\", \"full\", \"remaining_space\"]\n",
    "\n",
    "state_shaping_conf = {\n",
    "    \"look_back\": 7,\n",
    "    \"max_ports_downstream\": 2\n",
    "}\n",
    "\n",
    "action_shaping_conf = {\n",
    "    \"action_space\": [(i - 10) / 10 for i in range(21)],\n",
    "    \"finite_vessel_space\": True,\n",
    "    \"has_early_discharge\": True\n",
    "}\n",
    "\n",
    "reward_shaping_conf = {\n",
    "    \"time_window\": 99,\n",
    "    \"fulfillment_factor\": 1.0,\n",
    "    \"shortage_factor\": 1.0,\n",
    "    \"time_decay\": 0.97\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Sampler\n",
    "\n",
    "An environment sampler defines state, action and reward shaping logic so that policies can interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from maro.rl.rollout import AbsEnvSampler\n",
    "from maro.simulator.scenarios.cim.common import Action, ActionType\n",
    "\n",
    "\n",
    "class CIMEnvSampler(AbsEnvSampler):\n",
    "    def _get_global_and_agent_state(self, event, tick=None):\n",
    "        \"\"\"\n",
    "        The state vector includes shortage and remaining vessel space over the past k days (where k is the \"look_back\"\n",
    "        value in \"state_shaping_conf\" from the cell above), as well as all downstream port features.\n",
    "        \"\"\"\n",
    "        tick = self._env.tick\n",
    "        vessel_snapshots, port_snapshots = self._env.snapshot_list[\"vessels\"], self._env.snapshot_list[\"ports\"]\n",
    "        port_idx, vessel_idx = event.port_idx, event.vessel_idx\n",
    "        ticks = [max(0, tick - rt) for rt in range(state_shaping_conf[\"look_back\"] - 1)]\n",
    "        future_port_list = vessel_snapshots[tick: vessel_idx: 'future_stop_list'].astype('int')\n",
    "        state = np.concatenate([\n",
    "            port_snapshots[ticks: [port_idx] + list(future_port_list): port_attributes],\n",
    "            vessel_snapshots[tick: vessel_idx: vessel_attributes]\n",
    "        ])\n",
    "        return state, {port_idx: state}\n",
    "\n",
    "    def _translate_to_env_action(self, action_dict, event):\n",
    "        \"\"\"\n",
    "        The policy output is an integer from [0, 20] which is to be interpreted as the index of \"action_space\" in\n",
    "        \"action_shaping_conf\" from the cell above. For example, action 5 corresponds to -0.5, which means loading\n",
    "        50% of the containers available at the current port to the vessel, while action 18 corresponds to 0.8, which\n",
    "        means loading 80% of the containers on the vessel to the port. Note that action 10 corresponds 0.0, which\n",
    "        means doing nothing. \n",
    "        \"\"\"\n",
    "        action_space = action_shaping_conf[\"action_space\"]\n",
    "        finite_vsl_space = action_shaping_conf[\"finite_vessel_space\"]\n",
    "        has_early_discharge = action_shaping_conf[\"has_early_discharge\"]\n",
    "\n",
    "        port_idx, model_action = list(action_dict.items()).pop()\n",
    "\n",
    "        vsl_idx, action_scope = event.vessel_idx, event.action_scope\n",
    "        vsl_snapshots = self._env.snapshot_list[\"vessels\"]\n",
    "        vsl_space = vsl_snapshots[self._env.tick:vsl_idx:vessel_attributes][2] if finite_vsl_space else float(\"inf\")\n",
    "\n",
    "        percent = abs(action_space[model_action[0]])\n",
    "        zero_action_idx = len(action_space) / 2  # index corresponding to value zero.\n",
    "        if model_action < zero_action_idx:\n",
    "            action_type = ActionType.LOAD\n",
    "            actual_action = min(round(percent * action_scope.load), vsl_space)\n",
    "        elif model_action > zero_action_idx:\n",
    "            action_type = ActionType.DISCHARGE\n",
    "            early_discharge = vsl_snapshots[self._env.tick:vsl_idx:\"early_discharge\"][0] if has_early_discharge else 0\n",
    "            plan_action = percent * (action_scope.discharge + early_discharge) - early_discharge\n",
    "            actual_action = round(plan_action) if plan_action > 0 else round(percent * action_scope.discharge)\n",
    "        else:\n",
    "            actual_action, action_type = 0, ActionType.LOAD\n",
    "\n",
    "        return {port_idx: Action(vsl_idx, int(port_idx), actual_action, action_type)}\n",
    "\n",
    "    def _get_reward(self, env_action_dict, event, tick):\n",
    "        \"\"\"\n",
    "        The reward is defined as a linear combination of fulfillment and shortage measures. The fulfillment and\n",
    "        shortage measure are the sums of fulfillment and shortage values over the next k days, respectively, each\n",
    "        adjusted with exponential decay factors (using the \"time_decay\" value in \"reward_shaping_conf\" from the\n",
    "        cell above) to put more emphasis on the near future. Here k is the \"time_window\" value in \"reward_shaping_conf\".\n",
    "        The linear combination coefficients are given by \"fulfillment_factor\" and \"shortage_factor\" in \"reward_shaping_conf\".\n",
    "        \"\"\"\n",
    "        start_tick = tick + 1\n",
    "        ticks = list(range(start_tick, start_tick + reward_shaping_conf[\"time_window\"]))\n",
    "\n",
    "        # Get the ports that took actions at the given tick\n",
    "        ports = [int(port) for port in list(env_action_dict.keys())]\n",
    "        port_snapshots = self._env.snapshot_list[\"ports\"]\n",
    "        future_fulfillment = port_snapshots[ticks:ports:\"fulfillment\"].reshape(len(ticks), -1)\n",
    "        future_shortage = port_snapshots[ticks:ports:\"shortage\"].reshape(len(ticks), -1)\n",
    "\n",
    "        decay_list = [reward_shaping_conf[\"time_decay\"] ** i for i in range(reward_shaping_conf[\"time_window\"])]\n",
    "        rewards = np.float32(\n",
    "            reward_shaping_conf[\"fulfillment_factor\"] * np.dot(future_fulfillment.T, decay_list)\n",
    "            - reward_shaping_conf[\"shortage_factor\"] * np.dot(future_shortage.T, decay_list)\n",
    "        )\n",
    "        return {agent_id: reward for agent_id, reward in zip(ports, rewards)}\n",
    "\n",
    "    def get_env_metrics(self) -> None:\n",
    "        \"\"\"\n",
    "        The environment sampler contains a \"_info\" attribute inherited from the \"AbsEnvSampler\" base class, which can\n",
    "        be used to record any information one wishes to keep track of during a roll-out episode. Here we simply\n",
    "        record the latest env metric without keeping the history for logging purposes.\n",
    "        \"\"\"\n",
    "        return self._env.metrics\n",
    "\n",
    "    def _post_step(self, cache_element, reward) -> None:\n",
    "        self._info[\"env_metric\"] = self._env.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Policies](https://maro.readthedocs.io/en/latest/key_components/rl_toolkit.html#policy)\n",
    "\n",
    "The out-of-the-box ActorCritic is used as our agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import Adam, RMSprop\n",
    "\n",
    "from maro.rl.model import DiscretePolicyNet, FullyConnected, VNet\n",
    "from maro.rl.policy import DiscretePolicyGradient\n",
    "from maro.rl.training.algorithms import DiscreteActorCritic, DiscreteActorCriticParams\n",
    "\n",
    "# We consider the port in question as well as two downstream ports.\n",
    "# We consider the states of these ports over the past 7 days plus the current day, hence the factor 8.\n",
    "# obtain state dimension from a temporary env_wrapper instance\n",
    "state_dim = (\n",
    "    (state_shaping_conf[\"look_back\"] + 1) * (state_shaping_conf[\"max_ports_downstream\"] + 1) * len(port_attributes)\n",
    "    + len(vessel_attributes)\n",
    ")\n",
    "action_num = len(action_shaping_conf[\"action_space\"])\n",
    "\n",
    "# AC settings\n",
    "actor_net_conf = {\n",
    "    \"input_dim\": state_dim,\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"output_dim\": len(action_shaping_conf[\"action_space\"]),\n",
    "    \"activation\": torch.nn.Tanh,\n",
    "    \"softmax\": True,\n",
    "    \"batch_norm\": False,\n",
    "    \"head\": True\n",
    "}\n",
    "critic_net_conf = {\n",
    "    \"input_dim\": state_dim,\n",
    "    \"hidden_dims\": [256, 128, 64],\n",
    "    \"output_dim\": 1,\n",
    "    \"activation\": torch.nn.LeakyReLU,\n",
    "    \"softmax\": False,\n",
    "    \"batch_norm\": True,\n",
    "    \"head\": True\n",
    "}\n",
    "\n",
    "actor_optim_conf = (Adam, {\"lr\": 0.001})\n",
    "critic_optim_conf = (RMSprop, {\"lr\": 0.001})\n",
    "\n",
    "ac_conf = DiscreteActorCriticParams(\n",
    "    get_v_critic_net_func=lambda: MyCriticNet(),\n",
    "    reward_discount=.0,\n",
    "    grad_iters=10,\n",
    "    critic_loss_cls=torch.nn.SmoothL1Loss,\n",
    "    min_logp=None,\n",
    "    lam=.0\n",
    ")\n",
    "\n",
    "\n",
    "class MyActorNet(DiscretePolicyNet):\n",
    "    def __init__(self) -> None:\n",
    "        super(MyActorNet, self).__init__(state_dim=state_dim, action_num=action_num)\n",
    "        self._actor = FullyConnected(**actor_net_conf)\n",
    "        self._optim = actor_optim_conf[0](self._actor.parameters(), **actor_optim_conf[1])\n",
    "\n",
    "    def _get_action_probs_impl(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        return self._actor(states)\n",
    "\n",
    "    def freeze(self) -> None:\n",
    "        self.freeze_all_parameters()\n",
    "\n",
    "    def unfreeze(self) -> None:\n",
    "        self.unfreeze_all_parameters()\n",
    "\n",
    "    def step(self, loss):\n",
    "        self._optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self._optim.step()\n",
    "\n",
    "    def get_gradients(self, loss: torch.Tensor):\n",
    "        self._optim.zero_grad()\n",
    "        loss.backward()\n",
    "        return {name: param.grad for name, param in self.named_parameters()}\n",
    "\n",
    "    def apply_gradients(self, grad: dict) -> None:\n",
    "        for name, param in self.named_parameters():\n",
    "            param.grad = grad[name]\n",
    "        self._optim.step()\n",
    "\n",
    "    def get_state(self) -> dict:\n",
    "        return {\n",
    "            \"network\": self.state_dict(),\n",
    "            \"optim\": self._optim.state_dict()\n",
    "        }\n",
    "\n",
    "    def set_state(self, net_state: dict) -> None:\n",
    "        self.load_state_dict(net_state[\"network\"])\n",
    "        self._optim.load_state_dict(net_state[\"optim\"])\n",
    "\n",
    "\n",
    "class MyCriticNet(VNet):\n",
    "    def __init__(self) -> None:\n",
    "        super(MyCriticNet, self).__init__(state_dim=state_dim)\n",
    "        self._critic = FullyConnected(**critic_net_conf)\n",
    "        self._optim = critic_optim_conf[0](self._critic.parameters(), **critic_optim_conf[1])\n",
    "\n",
    "    def _get_v_values(self, states: torch.Tensor) -> torch.Tensor:\n",
    "        return self._critic(states).squeeze(-1)\n",
    "\n",
    "    def step(self, loss: torch.Tensor):\n",
    "        self._optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self._optim.step()\n",
    "\n",
    "    def get_gradients(self, loss: torch.Tensor):\n",
    "        self._optim.zero_grad()\n",
    "        loss.backward()\n",
    "        return {name: param.grad for name, param in self.named_parameters()}\n",
    "\n",
    "    def apply_gradients(self, grad: dict) -> None:\n",
    "        for name, param in self.named_parameters():\n",
    "            param.grad = grad[name]\n",
    "        self._optim.step()\n",
    "\n",
    "    def get_state(self) -> dict:\n",
    "        return {\n",
    "            \"network\": self.state_dict(),\n",
    "            \"optim\": self._optim.state_dict()\n",
    "        }\n",
    "\n",
    "    def set_state(self, net_state: dict) -> None:\n",
    "        self.load_state_dict(net_state[\"network\"])\n",
    "        self._optim.load_state_dict(net_state[\"optim\"])\n",
    "\n",
    "    def freeze(self) -> None:\n",
    "        self.freeze_all_parameters()\n",
    "\n",
    "    def unfreeze(self) -> None:\n",
    "        self.unfreeze_all_parameters()\n",
    "\n",
    "policy_dict = {f\"ac_{i}.policy\": DiscretePolicyGradient(f\"ac_{i}.policy\", policy_net=MyActorNet()) for i in range(4)}\n",
    "trainer_creator = {f\"ac_{i}\": lambda name: DiscreteActorCritic(name, params=ac_conf) for i in range(4)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Loop\n",
    "\n",
    "This code cell demonstrates a typical single-threaded training workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assign policy ac_0.policy to device cpu\n",
      "Assign policy ac_1.policy to device cpu\n",
      "Assign policy ac_2.policy to device cpu\n",
      "Assign policy ac_3.policy to device cpu\n",
      "Policy ac_0.policy has already been assigned to cpu. No need to take further actions.\n",
      "Policy ac_1.policy has already been assigned to cpu. No need to take further actions.\n",
      "Policy ac_2.policy has already been assigned to cpu. No need to take further actions.\n",
      "Policy ac_3.policy has already been assigned to cpu. No need to take further actions.\n",
      "env summary (episode 1, segment 1): {'order_requirements': 1120000, 'container_shortage': 755120, 'operation_number': 2003666}\n",
      "env summary (episode 2, segment 1): {'order_requirements': 1120000, 'container_shortage': 574168, 'operation_number': 1921718}\n",
      "env summary (episode 3, segment 1): {'order_requirements': 1120000, 'container_shortage': 467303, 'operation_number': 1957221}\n",
      "env summary (episode 4, segment 1): {'order_requirements': 1120000, 'container_shortage': 287538, 'operation_number': 1982078}\n",
      "env summary (episode 5, segment 1): {'order_requirements': 1120000, 'container_shortage': 266790, 'operation_number': 2017310}\n",
      "env summary (episode 6, segment 1): {'order_requirements': 1120000, 'container_shortage': 207813, 'operation_number': 1993367}\n",
      "env summary (episode 7, segment 1): {'order_requirements': 1120000, 'container_shortage': 258700, 'operation_number': 1924868}\n",
      "env summary (episode 8, segment 1): {'order_requirements': 1120000, 'container_shortage': 282732, 'operation_number': 1909072}\n",
      "env summary (episode 9, segment 1): {'order_requirements': 1120000, 'container_shortage': 271406, 'operation_number': 1836347}\n",
      "env summary (episode 10, segment 1): {'order_requirements': 1120000, 'container_shortage': 253359, 'operation_number': 1797864}\n",
      "env summary (episode 11, segment 1): {'order_requirements': 1120000, 'container_shortage': 285019, 'operation_number': 1764610}\n",
      "env summary (episode 12, segment 1): {'order_requirements': 1120000, 'container_shortage': 290217, 'operation_number': 1775871}\n",
      "env summary (episode 13, segment 1): {'order_requirements': 1120000, 'container_shortage': 291317, 'operation_number': 1696991}\n",
      "env summary (episode 14, segment 1): {'order_requirements': 1120000, 'container_shortage': 363557, 'operation_number': 1584546}\n",
      "env summary (episode 15, segment 1): {'order_requirements': 1120000, 'container_shortage': 276197, 'operation_number': 1702276}\n",
      "env summary (episode 16, segment 1): {'order_requirements': 1120000, 'container_shortage': 449572, 'operation_number': 1343822}\n",
      "env summary (episode 17, segment 1): {'order_requirements': 1120000, 'container_shortage': 569588, 'operation_number': 1060983}\n",
      "env summary (episode 18, segment 1): {'order_requirements': 1120000, 'container_shortage': 477787, 'operation_number': 1269050}\n",
      "env summary (episode 19, segment 1): {'order_requirements': 1120000, 'container_shortage': 675626, 'operation_number': 832979}\n",
      "env summary (episode 20, segment 1): {'order_requirements': 1120000, 'container_shortage': 678845, 'operation_number': 809899}\n"
     ]
    }
   ],
   "source": [
    "from maro.rl.rollout import SimpleAgentWrapper\n",
    "from maro.rl.training import TrainerManager \n",
    "from maro.simulator import Env\n",
    "from maro.utils import set_seeds\n",
    "\n",
    "set_seeds(1024)  # for reproducibility\n",
    "\n",
    "agent2policy = {agent: f\"ac_{agent}.policy\" for agent in Env(**env_conf).agent_idx_list}\n",
    "\n",
    "# The env sampler and trainer manager both take ``policy_creator`` as a parameter. The policy creator is\n",
    "# a function that takes a name and returns a policy instance. This design is convenient in distributed\n",
    "# settings where policies need to be created on both the training side and the roll-out (inference) side\n",
    "# and policy states need to be transferred from the former to the latter at the start of each roll-out\n",
    "# episode. Here we are demonstrating a single-threaded workflow where there is only one instance of each\n",
    "# policy, so we use a little trick here to ensure that the policies created inside the env sampler and the\n",
    "# training manager point the the same instances. \n",
    "policy_creator = {name: lambda name: policy_dict[name] for name in policy_dict}\n",
    "\n",
    "env_sampler = CIMEnvSampler(\n",
    "    get_env=lambda: Env(**env_conf),\n",
    "    policy_creator=policy_creator,\n",
    "    agent2policy=agent2policy,\n",
    "    agent_wrapper_cls=SimpleAgentWrapper,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "trainer_manager = TrainerManager(policy_creator, trainer_creator, agent2policy)\n",
    "\n",
    "# main loop with 50 episodes\n",
    "for ep in range(1, 21):\n",
    "    collect_time = training_time = 0\n",
    "    segment, end_of_episode = 1, False\n",
    "    while not end_of_episode:\n",
    "        # experience collection\n",
    "        result = env_sampler.sample()\n",
    "        experiences = result[\"experiences\"]\n",
    "        end_of_episode: bool = result[\"end_of_episode\"]\n",
    "        print(f\"env summary (episode {ep}, segment {segment}): {env_sampler.get_env_metrics()}\")\n",
    "        trainer_manager.record_experiences(experiences)\n",
    "        trainer_manager.train()\n",
    "        segment += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8f57a09d39b50edfb56e79199ef40583334d721b06ead0e38a39e7e79092073c"
  },
  "kernelspec": {
   "display_name": "maro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
