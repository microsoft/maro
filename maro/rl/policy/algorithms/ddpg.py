# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from typing import Union

import numpy as np
import torch

from maro.rl.experience import ExperienceStore, UniformSampler
from maro.rl.exploration import GaussianNoiseExploration
from maro.rl.model import ContinuousACNet
from maro.rl.policy import AbsCorePolicy
from maro.rl.utils import get_torch_loss_cls


class DDPGConfig:
    """Configuration for the DDPG algorithm.

    Args:
        reward_discount (float): Reward decay as defined in standard RL terminology.
        target_update_freq (int): Number of training rounds between policy target model updates.
        train_epochs (int): Number of training epochs per call to ``update()``. Defaults to 1.
        q_value_loss_cls: A string indicating a loss class provided by torch.nn or a custom loss class for
            the Q-value loss. If it is a string, it must be a key in ``TORCH_LOSS``. Defaults to "mse".
        policy_loss_coefficient (float): The coefficient for policy loss in the total loss function, e.g.,
            loss = q_value_loss + ``policy_loss_coefficient`` * policy_loss. Defaults to 1.0.
        soft_update_coefficient (float): Soft update coefficient, e.g.,
            target_model = (soft_update_coefficient) * eval_model + (1-soft_update_coefficient) * target_model.
            Defaults to 1.0.
    """
    __slots__ = [
        "reward_discount", "target_update_freq", "train_epochs", "q_value_loss_func", "policy_loss_coefficient",
        "soft_update_coefficient"
    ]

    def __init__(
        self,
        reward_discount: float,
        target_update_freq: int,
        train_epochs: int = 1,
        q_value_loss_cls="mse",
        policy_loss_coefficient: float = 1.0,
        soft_update_coefficient: float = 1.0,
    ):
        self.reward_discount = reward_discount
        self.target_update_freq = target_update_freq
        self.train_epochs = train_epochs
        self.q_value_loss_func = get_torch_loss_cls(q_value_loss_cls)()
        self.policy_loss_coefficient = policy_loss_coefficient
        self.soft_update_coefficient = soft_update_coefficient


class DDPG(AbsCorePolicy):
    """The Deep Deterministic Policy Gradient (DDPG) algorithm.

    References:
        https://arxiv.org/pdf/1509.02971.pdf
        https://github.com/openai/spinningup/tree/master/spinup/algos/pytorch/ddpg

    Args:
        ac_net (ContinuousACNet): DDPG policy and q-value models.
        config (DDPGConfig): Configuration for DDPG algorithm.
        experience_store (ExperienceStore): An ``ExperienceStore`` instance for storing and retrieving experiences
            generated by the policy.
        experience_sampler_cls: Type of experience sampler. Must be a subclass of ``AbsSampler``. Defaults to
            ``UnifromSampler``.
        experience_sampler_kwargs (dict): Keyword arguments for ``experience_sampler_cls``.
        post_step (Callable): Custom function to be called after each gradient step. This can be used for tracking
            the learning progress. The function should have signature (loss, tracker) -> None. Defaults to None.
    """
    def __init__(
        self,
        ac_net: ContinuousACNet,
        config: DDPGConfig,
        experience_store: ExperienceStore,
        experience_sampler_cls=UniformSampler,
        experience_sampler_kwargs: dict = {},
        exploration=GaussianNoiseExploration(),
        post_step=None
    ):
        if not isinstance(ac_net, ContinuousACNet):
            raise TypeError("model must be an instance of 'ContinuousACNet'")

        super().__init__(
            experience_store,
            experience_sampler_cls=experience_sampler_cls,
            experience_sampler_kwargs=experience_sampler_kwargs,
            exploration=exploration
        )
        self.ac_net = ac_net
        if self.ac_net.trainable:
            self.target_ac_net = ac_net.copy()
            self.target_ac_net.eval()
        else:
            self.target_ac_net = None
        self.config = config
        self._post_step = post_step
        self.device = self.ac_net.device
        self._num_steps = 0

    def choose_action(self, states) -> Union[float, np.ndarray]:
        self.ac_net.eval()
        with torch.no_grad():
            actions = self.ac_net.get_action(states).cpu().numpy()

        return actions[0] if len(actions) == 1 else actions

    def learn(self):
        assert self.ac_net.trainable, "ac_net needs to have at least one optimizer registered."
        self.ac_net.train()
        for _ in range(self.config.train_epochs):
            experience_set = self.sampler.get()
            states, next_states = experience_set.states, experience_set.next_states
            actual_actions = torch.from_numpy(experience_set.actions).to(self.device)
            rewards = torch.from_numpy(experience_set.rewards).to(self.device)
            if len(actual_actions.shape) == 1:
                actual_actions = actual_actions.unsqueeze(dim=1)  # (N, 1)

            with torch.no_grad():
                next_q_values = self.target_ac_net.value(next_states)
            target_q_values = (rewards + self.config.reward_discount * next_q_values).detach()  # (N,)

            q_values = self.ac_net(states, actions=actual_actions).squeeze(dim=1)  # (N,)
            q_value_loss = self.config.q_value_loss_func(q_values, target_q_values)
            policy_loss = -self.ac_net.value(states).mean()
            loss = q_value_loss + self.config.policy_loss_coefficient * policy_loss
            self.ac_net.step(loss)
            if self._post_step:
                self._post_step(loss.detach().cpu().numpy(), self.tracker)

            self._num_steps += 1
            if self._num_steps % self.config.target_update_freq == 0:
                self.target_ac_net.soft_update(self.ac_net, self.config.soft_update_coefficient)

    def set_state(self, policy_state):
        self.ac_net.load_state_dict(policy_state)
        self.target_ac_net = self.ac_net.copy() if self.ac_net.trainable else None

    def get_state(self):
        return self.ac_net.state_dict()
