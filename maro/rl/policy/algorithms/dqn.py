# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from typing import Callable, Union

import numpy as np
import torch

from maro.rl.experience import ExperienceStore, PrioritizedSampler, UniformSampler
from maro.rl.exploration import DiscreteSpaceExploration, EpsilonGreedyExploration
from maro.rl.model import DiscreteQNet
from maro.rl.policy import AbsCorePolicy


class DQNConfig:
    """Configuration for the DQN algorithm.

    Args:
        reward_discount (float): Reward decay as defined in standard RL terminology.
        update_target_every (int): Number of gradient steps between target model updates.
        train_epochs (int): Number of training epochs per call to ``update()``. Defaults to 1.
        soft_update_coeff (float): Soft update coefficient, e.g.,
            target_model = (soft_update_coeff) * eval_model + (1-soft_update_coeff) * target_model.
            Defaults to 1.0.
        double (bool): If True, the next Q values will be computed according to the double DQN algorithm,
            i.e., q_next = Q_target(s, argmax(Q_eval(s, a))). Otherwise, q_next = max(Q_target(s, a)).
            See https://arxiv.org/pdf/1509.06461.pdf for details. Defaults to False.
    """
    __slots__ = ["reward_discount", "update_target_every", "train_epochs", "soft_update_coeff", "double"]

    def __init__(
        self,
        reward_discount: float,
        update_target_every: int,
        train_epochs: int = 1,
        soft_update_coeff: float = 0.1,
        double: bool = True
    ):
        self.reward_discount = reward_discount
        self.update_target_every = update_target_every
        self.train_epochs = train_epochs
        self.soft_update_coeff = soft_update_coeff
        self.double = double


class DQN(AbsCorePolicy):
    """The Deep-Q-Networks algorithm.

    See https://web.stanford.edu/class/psych209/Readings/MnihEtAlHassibis15NatureControlDeepRL.pdf for details.

    Args:
        q_net (DiscreteQNet): Q-value model.
        config (DQNConfig): Configuration for DQN algorithm.
        experience_store (ExperienceStore): An ``ExperienceStore`` instance for storing and retrieving experiences
            generated by the policy.
        experience_sampler_cls: Type of experience sampler. Must be a subclass of ``AbsSampler``. Defaults to
            ``UnifromSampler``.
        experience_sampler_kwargs (dict): Keyword arguments for ``experience_sampler_cls``.
        exploration (DiscreteSpaceExploration): Exploration strategy for generating exploratory actions. Defaults to
            an ``EpsilonGreedyExploration`` instance.
        post_step (Callable): Custom function to be called after each gradient step. This can be used for tracking
            the learning progress. The function should have signature (loss, tracker) -> None. Defaults to None.
    """
    def __init__(
        self,
        q_net: DiscreteQNet,
        config: DQNConfig,
        experience_store: ExperienceStore,
        experience_sampler_cls=UniformSampler,
        experience_sampler_kwargs: dict = {},
        exploration: DiscreteSpaceExploration = EpsilonGreedyExploration(),
        post_step: Callable = None
    ):
        if not isinstance(q_net, DiscreteQNet):
            raise TypeError("model must be an instance of 'DiscreteQNet'")

        super().__init__(
            experience_store,
            experience_sampler_cls=experience_sampler_cls,
            experience_sampler_kwargs=experience_sampler_kwargs,
            exploration=exploration
        )
        self.q_net = q_net
        if self.q_net.trainable:
            self.target_q_net = q_net.copy()
            self.target_q_net.eval()
        else:
            self.target_q_net = None
        self.config = config
        self._post_step = post_step
        self.device = self.q_net.device
        self._num_steps = 0

        self.prioritized_experience_replay = isinstance(self.sampler, PrioritizedSampler)
        if not self.prioritized_experience_replay:
            self._loss_func = torch.nn.MSELoss()

    def choose_action(self, states) -> Union[int, np.ndarray]:
        self.q_net.eval()
        with torch.no_grad():
            q_for_all_actions = self.q_net(states)  # (batch_size, num_actions)
            _, actions = q_for_all_actions.max(dim=1)

        actions = actions.cpu().numpy()
        if self.exploration.action_space is None:
            self.exploration.set_action_space(np.arange(q_for_all_actions.shape[1]))
        if self.exploring:
            actions = self.exploration(actions, state=states)
        return actions[0] if len(actions) == 1 else actions

    def learn(self):
        assert self.q_net.trainable, "q_net needs to have at least one optimizer registered."
        self.q_net.train()
        for _ in range(self.config.train_epochs):
            # sample from the replay memory
            experience_set = self.sampler.get()
            states, next_states = experience_set.states, experience_set.next_states
            actions = torch.from_numpy(np.asarray(experience_set.actions)).to(self.device)
            rewards = torch.from_numpy(np.asarray(experience_set.rewards)).to(self.device)
            if self.prioritized_experience_replay:
                indexes = [info["index"] for info in experience_set.info]
                is_weights = torch.tensor([info["is_weight"] for info in experience_set.info]).to(self.device)

            # get target Q values
            with torch.no_grad():
                if self.config.double:
                    actions_by_eval_q_net = self.q_net.get_action(next_states)[0]
                    next_q_values = self.target_q_net.q_values(next_states, actions_by_eval_q_net)
                else:
                    next_q_values = self.target_q_net.get_action(next_states)[1]  # (N,)

            target_q_values = (rewards + self.config.reward_discount * next_q_values).detach()  # (N,)

            # gradient step
            q_values = self.q_net.q_values(states, actions)
            if self.prioritized_experience_replay:
                td_errors = target_q_values - q_values
                loss = (td_errors * is_weights).mean()
                self.sampler.update(indexes, td_errors.detach().cpu().numpy())
            else:
                loss = self._loss_func(q_values, target_q_values)
            self.q_net.step(loss)

            if self._post_step:
                self._post_step(loss.detach().cpu().numpy(), self.tracker)

            # soft-update target network
            self._num_steps += 1
            if self._num_steps % self.config.update_target_every == 0:
                self.target_q_net.soft_update(self.q_net, self.config.soft_update_coeff)

    def get_loss(self):
        '''Sample data and return loss.
        '''
        assert self.q_net.trainable, "q_net needs to have at least one optimizer registered."
        self.q_net.train()
        # TODO: for _ in range(self.config.train_epochs):
        # sample from the replay memory
        experience_set = self.sampler.get()
        states, next_states = experience_set.states, experience_set.next_states
        actions = torch.from_numpy(np.asarray(experience_set.actions)).to(self.device)
        rewards = torch.from_numpy(np.asarray(experience_set.rewards)).to(self.device)
        if self.prioritized_experience_replay:
            indexes = [info["index"] for info in experience_set.info]
            is_weights = torch.tensor([info["is_weight"] for info in experience_set.info]).to(self.device)

        # get target Q values
        with torch.no_grad():
            if self.config.double:
                actions_by_eval_q_net = self.q_net.get_action(next_states)[0]
                next_q_values = self.target_q_net.q_values(next_states, actions_by_eval_q_net)
            else:
                next_q_values = self.target_q_net.get_action(next_states)[1]  # (N,)

        target_q_values = (rewards + self.config.reward_discount * next_q_values).detach()  # (N,)

        # gradient step
        q_values = self.q_net.q_values(states, actions)
        if self.prioritized_experience_replay:
            td_errors = target_q_values - q_values
            loss = (td_errors * is_weights).mean()
            self.sampler.update(indexes, td_errors.detach().cpu().numpy())
        else:
            loss = self._loss_func(q_values, target_q_values)

        if self._post_step:
            self._post_step(loss.detach().cpu().numpy(), self.tracker)
        return loss

    def step(self, grad_dict):
        '''Backward step.'''
        # set gradient & optimize
        for name, param in self.q_net.named_parameters():
            param.grad = grad_dict[name]
        self.q_net.optimizer.step()

        # soft-update target network
        self._num_steps += 1
        if self._num_steps % self.config.update_target_every == 0:
            self.target_q_net.soft_update(self.q_net, self.config.soft_update_coeff)

    def set_state(self, policy_state):
        self.q_net.load_state_dict(policy_state)
        self.target_q_net = self.q_net.copy() if self.q_net.trainable else None
        if self.target_q_net:
            self.target_q_net.eval()

    def get_state(self):
        return self.q_net.state_dict()
