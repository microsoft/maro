# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from os import getcwd
from typing import List, Union

from maro.utils import Logger

from .policy_manager import AbsPolicyManager
from .rollout_manager import AbsRolloutManager


class Learner:
    """Main controller for learning.

    Args:
        policy_manager (AbsPolicyManager): An ``AbsPolicyManager`` instance that controls policy updates.
        rollout_manager (AbsRolloutManager): An ``AbsRolloutManager`` instance that controls simulation data
            collection.
        num_episodes (int): Number of training episodes. Each training episode may contain one or more
            collect-update cycles, depending on how the implementation of the roll-out manager.
        eval_schedule (Union[int, List[int]]): Evaluation schedule. If an integer is provided, the policies will
            will be evaluated every ``eval_schedule`` episodes. If a list is provided, the policies will be evaluated
            at the end of the training episodes given in the list. In any case, the policies will be evaluated
            at the end of the last training episode. Defaults to None, in which case the policies will only be
            evaluated after the last training episode.
        log_dir (str): Directory to store logs in. A ``Logger`` with tag "LEARNER" will be created at init time
            and this directory will be used to save the log files generated by it. Defaults to the current working
            directory.
        end_of_episode_kwargs: Keyword arguments for custom end-of-episode processing.
    """
    def __init__(
        self,
        policy_manager: AbsPolicyManager,
        rollout_manager: AbsRolloutManager,
        num_episodes: int,
        eval_schedule: Union[int, List[int]] = None,
        log_dir: str = getcwd(),
        **end_of_episode_kwargs
    ):
        self.logger = Logger("LEARNER", dump_folder=log_dir)
        self.policy_manager = policy_manager
        self.rollout_manager = rollout_manager

        self.num_episodes = num_episodes

        # evaluation schedule
        if eval_schedule is None:
            eval_schedule = []
        elif isinstance(eval_schedule, int):
            num_eval_schedule = num_episodes // eval_schedule
            eval_schedule = [eval_schedule * i for i in range(1, num_eval_schedule + 1)]

        self._eval_schedule = eval_schedule
        self._eval_schedule.sort()
        if not self._eval_schedule or num_episodes != self._eval_schedule[-1]:
            self._eval_schedule.append(num_episodes)

        self.logger.info(f"Policy will be evaluated at the end of episodes {self._eval_schedule}")
        self._eval_point_index = 0

        self._end_of_episode_kwargs = end_of_episode_kwargs
        self._updated_policy_ids = self.policy_manager.names
        self._last_step_set = {}

    def run(self):
        """Entry point for executing a learning workflow."""
        for ep in range(1, self.num_episodes + 1):
            self._train(ep)
            if ep == self._eval_schedule[self._eval_point_index]:
                self._eval_point_index += 1
                self.rollout_manager.evaluate(self.policy_manager.get_state())

    def _train(self, ep: int):
        num_experiences_collected = 0
        segment = 0
        self.rollout_manager.reset()
        while not self.rollout_manager.episode_complete:
            segment += 1
            # experience collection
            policy_state_dict = self.policy_manager.get_state()
            exp_by_agent = self.rollout_manager.collect(ep, segment, policy_state_dict)
            self.policy_manager.on_experiences(exp_by_agent)
            num_experiences_collected += sum(exp.size for exp in exp_by_agent.values())

        # performance details
        self.logger.debug(f"ep {ep} summary - experiences collected: {num_experiences_collected}")

        self.end_of_episode(ep, **self._end_of_episode_kwargs)

    def end_of_episode(self, ep: int, **kwargs):
        """Custom end-of-episode processing is implemented here."""
        pass
