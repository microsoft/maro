# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

from collections import defaultdict
from os import getcwd
from typing import Dict

from maro.communication import Proxy
from maro.rl.env_wrapper import AbsEnvWrapper
from maro.rl.exploration import AbsExploration
from maro.utils import Logger

from .decision_generator import AbsDecisionGenerator
from .message_enums import MsgKey, MsgTag


class Actor(object):
    """On-demand roll-out worker.

    Args:
        env (AbsEnvWrapper): An ``AbsEnvWrapper`` instance to interact with a set of agents and collect experiences
            for policy training / update.
        decision_generator (AbsDecisionGenerator): Source of action decisions which could be local or remote
            depending on the implementation. 
        group (str): Identifier of the group to which the actor belongs. It must be the same group name
            assigned to the learner (and decision clients, if any).
        exploration_dict (Dict[str, AbsExploration]): A set of named exploration schemes. Defaults to None.
        agent2exploration (Dict[str, str]): Mapping from agent ID's to exploration scheme ID's. This is used to direct
            an agent's query to the correct exploration scheme. Defaults to None.
        eval_env (AbsEnvWrapper): An ``AbsEnvWrapper`` instance for policy evaluation. If None, ``env`` will be used
            as the evaluation environment. Defaults to None.
        log_dir (str): Directory to store logs in. A ``Logger`` will be created at init time and this directory
            will be used to save the log files generated by it. Defaults to the current working directory.
        proxy_kwargs: Keyword parameters for the internal ``Proxy`` instance. See ``Proxy`` class
            for details.
    """
    def __init__(
        self,
        env: AbsEnvWrapper,
        decision_generator: AbsDecisionGenerator,
        group: str,
        exploration_dict: Dict[str, AbsExploration] = None,
        agent2exploration: Dict[str, str] = None,
        eval_env: AbsEnvWrapper = None,
        log_dir: str = getcwd(),
        **proxy_kwargs
    ):
        self.env = env
        self.eval_env = eval_env if eval_env else self.env

        self.decision_generator = decision_generator

        # mappings between exploration schemes and agents
        self.exploration_dict = exploration_dict
        if exploration_dict:
            self.agent2exploration = agent2exploration
            self.exploration = {
                agent_id: self.exploration_dict[exploration_id]
                for agent_id, exploration_id in self.agent2exploration.items()
            }
            self.exploration_enabled = True
            self.agent_groups_by_exploration = defaultdict(list)
            for agent_id, exploration_id in agent2exploration.items():
                self.agent_groups_by_exploration[exploration_id].append(agent_id)

            for exploration_id, agent_ids in self.agent_groups_by_exploration.items():
                self.agent_groups_by_exploration[exploration_id] = tuple(agent_ids)

        self._proxy = Proxy(group, "actor", {"rollout_manager": 1}, **proxy_kwargs)
        self._logger = Logger(self._proxy.name, dump_folder=log_dir)

    def run(self):
        """Start the event loop.

        The event loop handles 3 types of messages from the roll-out manager:
            1)  COLLECT, upon which the agent-environment simulation will be carried out for a specified number of steps
                and the collected experiences will be sent back to the roll-out manager;
            2)  EVAL, upon which the policies contained in the message payload will be evaluated for the entire
                duration of the evaluation environment.
            3)  EXIT, upon which the actor will break out of the event loop and the process will terminate.

        """
        for msg in self._proxy.receive():
            if msg.tag == MsgTag.EXIT:
                self._logger.info("Exiting...")
                self._proxy.close()
                break

            if msg.tag == MsgTag.COLLECT:
                self._collect(msg)
            elif msg.tag == MsgTag.EVAL:
                self._evaluate(msg)

    def _collect(self, msg):
        ep, segment = msg.body[MsgKey.EPISODE], msg.body[MsgKey.SEGMENT]
        # load policies
        if hasattr(self.decision_generator, "update"):
            self.decision_generator.update(msg.body[MsgKey.POLICY])
        # set exploration parameters
        exploration_params = None
        if MsgKey.EXPLORATION in msg.body:
            updated_exploration_param_names = {}
            for exploration_name, param_dict in msg.body[MsgKey.EXPLORATION].items():
                updated_exploration_param_names[exploration_name] = param_dict.keys()
                for param_name, value in param_dict.items():
                    setattr(self.exploration_dict[exploration_name], param_name, value)

            exploration_params = {
                agent_ids: {
                    param_name: getattr(self.exploration_dict[exploration_name], param_name)
                    for param_name in updated_exploration_param_names[exploration_name]
                }
                for exploration_name, agent_ids in self.agent_groups_by_exploration.items()
            }

        if self.env.state is None:
            self._logger.info(f"Training episode {msg.body[MsgKey.EPISODE]}")
            if hasattr(self, "exploration_dict"): 
                self._logger.info(f"Exploration parameters: {exploration_params}")

            self.env.reset()
            self.env.start()  # get initial state

        starting_step_index = self.env.step_index + 1
        steps_to_go = float("inf") if msg.body[MsgKey.NUM_STEPS] == -1 else msg.body[MsgKey.NUM_STEPS]
        while self.env.state and steps_to_go > 0:
            action = self.decision_generator.choose_action(self.env.state, ep, self.env.step_index)
            if self.exploration_dict:
                for agent_id, exploration in self.exploration_dict.items():
                    action[agent_id] = exploration(action[agent_id])
            self.env.step(action)
            steps_to_go -= 1

        self._logger.info(
            f"Roll-out finished for ep {ep}, segment {segment}"
            f"(steps {starting_step_index} - {self.env.step_index})"
        )

        if hasattr(self.decision_generator, "store_experiences"):
            policy_names = self.decision_generator.store_experiences(self.env.get_experiences())
            ret_exp = self.decision_generator.get_experiences(policy_names)

        return_info = {
            MsgKey.EPISODE_END: not self.env.state,
            MsgKey.EPISODE: ep,
            MsgKey.SEGMENT: segment,
            MsgKey.EXPERIENCES: ret_exp,
            MsgKey.ENV_SUMMARY: self.env.summary,
            MsgKey.NUM_STEPS: self.env.step_index - starting_step_index + 1
        }

        self._proxy.reply(msg, tag=MsgTag.COLLECT_DONE, body=return_info)

    def _evaluate(self, msg):
        self._logger.info(f"Evaluating...")
        ep = msg.body[MsgKey.EPISODE]
        self.eval_env.reset()
        self.eval_env.start()  # get initial state
        if hasattr(self.decision_generator, "update"):
            self.decision_generator.update(msg.body[MsgKey.POLICY])
        while self.eval_env.state:
            action = self.decision_generator.choose_action(self.env.state, ep, self.eval_env.step_index)
            self.eval_env.step(action)

        return_info = {
            MsgKey.ENV_SUMMARY: self.env.summary,
            MsgKey.EPISODE: msg.body[MsgKey.EPISODE]
        }
        self._proxy.reply(msg, tag=MsgTag.EVAL_DONE, body=return_info)
