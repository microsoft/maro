# Copyright (c) Microsoft Corporation.
# Licensed under the MIT license.

job: your_job_name
# Path to a directory that defines a business scenario and contains the necessary components to execute reinforcement
# learning workflows in single-threaded, multi-process and distributed modes.
scenario_path: "/path/to/your/scenario"
log_path: "/path/to/your/log/folder"  # All logs are written to a single file for ease of viewing.
main:
  num_episodes: 100  # number of episodes to run. Each episode is one cycle of roll-out and training.
  # Number of environment steps to collect environment samples over. If null, samples are collected until the
  # environments reach the terminal state, i.e., for a full episode. Otherwise, samples are collected until the
  # specified number of steps or the terminal state is reached, whichever comes first.
  num_steps: null
  # This can be an integer or a list of integers. An integer indicates the interval at which policies are evaluated.
  # A list indicates the episodes at the end of which policies are to be evaluated.
  eval_schedule: 10
  logging:  # log levels for the main loop
    stdout: INFO  # DEBUG, INFO, WARN, ERROR, CRITICAL, PROGRESS
    file: DEBUG  # DEBUG, INFO, WARN, ERROR, CRITICAL, PROGRESS
rollout:
  # Optional section to specify roll-out parallelism settings. If not present, a single environment instance will be
  # created locally for training and evaluation.
  parallelism:
    sampling: 10  # Number of parallel roll-outs to collecting training data from.
    eval: null  # Number of parallel roll-outs to evaluate policies on. Defaults to 1 if not provided.
    # Minimum number of environment samples to collect from the parallel roll-outs. This value cannot exceed
    # rollout.parallelism.sampling and is ignored if rollout.parallelism.sampling == 1.
    min_env_samples: 8
    # Factor that determines the additional wait time after the required number of environment samples as indicated by
    # "min_env_samples" are received. For example, if T seconds elapsed after receiving "min_env_samples" environment
    # samples, it will wait an additional T * grace_factor seconds to try to collect the remaining results. This is
    # ignored if rollout.parallelism.sampling == 1.
    grace_factor: 0.2
    controller:  # Parallel roll-out controller settings. Ignored if rollout.parallelism section is absent.
      host: "127.0.0.1"  # Controller's IP address. Ignored if run in containerized environments.
      port: 20000  # Controller's network port for remote roll-out workers to connect to.
  logging:  # log levels for roll-out workers
    stdout: INFO  # DEBUG, INFO, WARN, ERROR, CRITICAL, PROGRESS
    file: DEBUG  # DEBUG, INFO, WARN, ERROR, CRITICAL, PROGRESS
training:
  # Must be "simple" or "parallel". In simple mode, all underlying models are trained locally. In parallel mode,
  # all trainers send gradient-related tasks to a proxy service where they get dispatched to a set of workers.
  mode: simple
  # Path to load previously saved trainer snapshots from. A policy trainer's snapshot includes the states of all
  # the policies it manages as well as the states of auxillary models (e.g., critics in the Actor-Critic paradigm).
  # If the path corresponds to an existing directory, the program will look under the directory for snapshot files
  # that match the trainer names specified in the scenario and attempt to load from them.
  load_path: "/path/to/your/models"
  checkpointing:
    # Directory to save trainer snapshots under. Snapshot files created at different episodes will be saved under
    # separate folders named using the episode number. For example, if a snapshot is created for a trainer named "dqn"
    # at the end of episode 10, the file path would be "/path/to/your/checkpoint/folder/10/dqn.ckpt".
    path: "/path/to/your/checkpoint/folder"
    interval: 10  # Interval at which trained policies / models are persisted to disk.
  proxy:  # Proxy settings. Ignored under simple mode.
    host: "127.0.0.1"  # Proxy service host's IP address. Ignored if run in containerized environments.
    frontend: 10000  # Proxy service's network port for trainers to send tasks to.
    backend: 10001  # Proxy service's network port for remote workers to connect to.
  num_workers: 10  # Number of workers to execute trainers' tasks.
  logging:  # log levels for training task workers
    stdout: INFO  # DEBUG, INFO, WARN, ERROR, CRITICAL, PROGRESS
    file: DEBUG  # DEBUG, INFO, WARN, ERROR, CRITICAL, PROGRESS
