## What's MARO RL

MARO reinforcement learning toolkits (MARO RL) is used to build the reinforcement learning workflows. It consists of three macro parts: 
- Policy: polices used by agents.
- Sampler: used to schedule the interaction between agents and the environment, and collect the interacting experiences.
- Training manager: used to train policies.

The following figure shows the overall structure. Next, we will explain the roles and functions of different components in RL separately.
![RL_workflow](picture/overview-RLworkflow.svg)

### What's Policy

Policy is a very important part of RL. It has specific policies in different scenarios and the associated products of agents. The following figure is the structure diagram of the policy part. In the framework of deep learning, many models regard agent and policy as unified objects. But this is not the case in MARO, it is the highest level abstraction of a policy object is `AbsPolicy`, the Policy Gradient in the figure can be understood as the final model scheme, which is a subclass that inherits `Abspolicy`, here we will involve Some important interfaces:

- `policy_net`: it is the base class for all core networks in the policy. Two different interfaces, `state_dim` and `action_dim`, are defined here, namely state and action. In addition there is `step` which runs a training step to update network parameters according to a given loss; `get_gradients` for all parameter gradients according to a given loss, etc.
- `MyActorNet`: Since Policynet is a general policy, different policy details need to be updated for different algorithms. This interface is used to update some details of the corresponding algorithm. For example `get_action_probs_impl`.

In Policy, there is a predetermined simplest policy `RuleBasedPolicy`, but it is mainly used in simple scenarios. However, complex scenarios require interactive training between the policy and the environment to obtain the optimal policy, so we define a ` RLPOLICY`.

In Maro's design concept, the strategy cannot be self-trained, and it needs to be trained collaboratively through the training components of the external part. Therefore, in addition to the state return interface, there is also a set of interface definitions related to training:
- `get_actions`: This is used to obtain the function of related actions according to the state, which derives several similar functions in specific algorithms such as `get_actions_with_probs`, `get_actions_with_logps`, etc.
- `get_gradients`: Gradients for all parameters can be obtained with a given loss (generic algorithm interface).
- `apply_gradients`: Apply gradients to the network to update all parameters.

These interfaces will be called by TrainingManager for training. Currently, in the Maro design assumption, all strategies are based on deep learning models, so the training-related interfaces are specially designed for gradient descent.

![policy_workflow](picture/overview-Policy.svg)


###  What's Sampler

Next comes the second component in RL - Sampler. It plays an important scheduling role in RL, interacting the policy with the environment and collecting the state and experience returned by the environment. This part is composed of environment simulator, Policy and collection experience components as shown in the following figure. Unlike Policy, sampler is an entity that inherits the base class of `AbsEnvSampler`, and `AbsEnvSampler` is mainly used to define the basic data collector and Policy Evaluator.

![sampler_workflow](picture/overview-Sampler.svg)


In Sampler, Systemtick (that is, the time stamp in the system) will continue to enter this component, and Policy will issue relevant Actions to the environment at a specific tick time to enter the environment simulator, and then the component that collects information will recycle the experience and The status is issued as such.

In this step, a base class `AbsEnvSampler` is defined in advance. There are some important functions in it. Only through them can the Sampler function be supported. Let's list a few to explain:
- `get_global_and_agent_state`: Used to obtain global and local agent state
- `get_reward`: Get rewards based on Env actions
- `translate_to_env_action`: Format the data and format the actions generated by the model into objects that the environment can execute These functions are only used as an interface in `AbsEnvSampler`, and the specific implementation is overridden by the sampler in a specific environment. (mentioned later in this section)

In MARO's design, since Policy and Agent are not actually an entity, but a sign similar to an index, a separate management component is required when using them. Here we propose to use `AbsAgentWrapper` for management. Manage agents and policies during experience gathering through some functional functions such as:
- `set_policy_state`: Used to directly set the state of the policy
- `choose_actions`: Select the corresponding action according to the state of the agent

After explaining the basic functions of `AbsEnvSampler` and the `AbsAgentWrapper` used for management, we need an intermediate scheduling component to run them together. At this time, we need to mention another function of `AbsEnvSampler`:

- `sample`: This is the scheduling function defined in `AbsEnvSampler`. Its function is to string together the interfaces of each function, interact with the environment and the policy, and then pass the returned information through `post_collect`.

Some of the functions mentioned above will be overwritten by the sampler in a specific environment. The following code is the basic definition of Sampler. In the definition, a specific Smapler needs to be declared and used, namely `EnvSampler` (here, `CIMEnvSampler` is used as an example to illustrate ).

```
env_sampler=CIMEnvSampler(
        learn_env=learn_env,
        test_env=test_env,
        policies=policies,
        agent2policy=agent2policy,
        reward_eval_delay=reward_shaping_conf["time_window"],
    )
```

`CIMEnvSampler` is a pre-defined scene in Maro's design, in addition to `VMEnvSampler`. They are Samplers with special words for different scenarios, in which functions such as `get_global_and_agent_state_impl`, `translate_to_env_action`, `get_reward` and other functions will be overwritten, so that they can have different effects from the current scene.


###  What's Training Manager

In the above introduction, we mentioned that Policy cannot self-update training in Maro, so it must be trained through external algorithms to update Policy, so the role of Training Manager is established. The following figure is the workflow, which collects the experience provided by the Sampler, and then transmits it to the training scheduler, and then pushes the train to train and update the policy. We will describe the composition of this component in detail below.

![training_mananger workflow](picture/overview-TrainingManager.svg)


#### Training Scheduler

The first thing we want to talk about is the TrainingScheduler component. It is an important scheduler in TrainingManager. There is a RwardHandle component in it, which is responsible for collecting the experience passed in the previous part. The main function involved is `record_experience()`. When the experience is collected, it is scheduled The trainer will execute `train_step()` again to send the experience to the core Trainer for training.

#### Trainer 

After learning about Training Scheduler, this part starts to get into Trainer. For any algorithm, we define the corresponding Params and OPS parts at the time of design. Here we first introduce the base class corresponding to the so-called Params part of the Params part is `TrainerParams`, which is the basic trainer parameter, among which Contains the interface `replay_memory_capacity()` for revisiting memory, the interface `batch_size()` for the batch size of each training, the interface `data_parallelism()` for data parallelism, and the interface `reward_discount()` for reward decay. Parameters are passed directly into the core algorithm.

In addition to parameters, another key is Ops, which is the basic minimum unit of training policy, mainly responsible for loss/gradient calculation and policy update, and each operation is used to train a single policy. The calculation logic related to the gradient is all implemented in ops.

The last part in Tarin is the algorithm part, which is also the core content of training. He combines the above two components to form Train. In Maro's design, the algorithm part provides a base class named `AbsTrainer`, which is mainly used to create the interface of the basic training strategy. The following three are more important. The interface is explained:
- `build`: The ops and memory required for create are more than the space used to create the call before training.
- `train_step`: This is an algorithm step scheduler that when called is able to run a training step to update all policies this trainer is responsible for.
- `record_multiple`:Record rollout all experiences from an environment in the replay memory.

Based on the design of `Abstrain`, two different modes are established for the correspondence between Policy and training algorithms:
- `SingleAgentTrainer`: This is a trainer that only trains one strategy, which can be understood as only one ops on the Trainer
- `MultiAgentTrainer`:  Similarly, `SingleAgentTrainer` can train a trainer of multiple policies. The trainer may contain multiple ops, which are responsible for the training of multiple policies and other auxiliary objects.

For the two modes, we have also improved different core algorithms, including：
- `ActorCriticTrainer`:Actor-critic algorithms for generating policies for discrete actions.
- `DDPGTrainer`: DDPG algorithm for generating policies for successive actions.
- `DQNTrainer`: DQN algorithm for generating policies for discrete actions.
- `DiscreteMADDPGTrainer`:MADDPG algorithm for generating policies for discrete actions.
- `PPOTrainer`: PPO algorithm for generating policies for discrete actions.
- `SoftActorCriticTrainer`：Soft actor-critic algorithm for generating policies for discrete actions

### RL Component Bundle 

The above is the introduction of all the components of the RL part. In addition, since there are three components interacting with each other, a resource management platform is established for management.

RL Component Bundle is a resource management component in this part of RL. There are multiple interfaces to link the defined Sampler, TrainingManager, and workflow. As shown in the following program, a collection is defined by `RLComponentBundle`, which contains `env_sampler `, `policies`, `trainers`, that is, the parts we mentioned above.

```
rl_component_bundle = RLComponentBundle(
    env_sampler=CIMEnvSampler(
        learn_env=learn_env,
        test_env=test_env,
        policies=policies,
        agent2policy=agent2policy,
        reward_eval_delay=reward_shaping_conf["time_window"],
    ),
    agent2policy=agent2policy,
    policies=policies,
    trainers=trainers,
)
```
