RL Toolkit
==========

MARO provides a full-stack abstraction for reinforcement learning (RL) which includes various customizable
components. In order to provide a gentle introduction for the RL toolkit, we cover the components in top-down
fashion, starting from the outermost layer, the learning workflows. The RL toolkit supports single-threaded and
distributed learning workflows. The distributed workflow can be synchronous or asynchronous.


Synchronous Learning
--------------------

In synchronous mode, a main process executes 2-phase learning cycles consisting of simulation data collection and
policy update. The data collection phase is controlled by a roll-out manager and the policy update phase is controlled
by a :ref:`policy-manager`. The transition from one phase to the other is synchrounous. To handle slow roll-out workers, the
roll-out manager can be configured to pass the results from a subset of roll-out workers (i.e., the faster ones) to the
policy manager. On the other hand, the policy manager always waits until all policies are updated before passing the
policy states to the roll-out manager.


.. figure:: ../images/rl/learning_cycle.svg
   :alt: Overview
   
   Synchronous Learning Cycle


.. figure:: ../images/rl/rollout_manager.svg
   :alt: Overview

   Roll-out Manager


Asynchronous Learning
---------------------

The asynchronous mode consists of a policy server and multiple actors in a client-server architecture. Each actor runs
its own roll-out loop and periodically sends the collected data to the server. The server uses the data to update the
policies and responds with the latest policy states. As the actors may differ significantly in speed, the policy server
only uses data generated by policies sufficiently up-to-date, but always sends the latest policy states to every single
actor. The asynchronous learning feature is still under development and not ready for production use, as it only
supports a single policy server and hence does not provide fault tolerence.


Environment Sampler
-------------------

It is necessary to implement an environment sampler (a subclass of ``AbsEnvSampler``) with user-defined state, action
and reward shaping to collect roll-out information for learning and testing purposes. An environment sampler can be
easily turned into a roll-out worker or an actor for synchronous and asynchronous learning, respectively.


.. figure:: ../images/rl/env_sampler.svg
   :alt: Overview

   Environment Sampler


Policy
------

The ``AbsPolicy`` abstraction is the core component of MARO's RL toolkit. A policy is a an agent's mechanism to select
actions in its interaction with an environment. MARO allows arbitrary agent-to-policy mapping to make policy sharing
easily configurable. It is also possible to use a mixture of rule-based policies and ``RLPolicy`` instances. The latter
provides various policy improvement interfaces to support single-threaded and distributed learning.   


.. code-block:: python

  class AbsPolicy(ABC):
      def __init__(self, name)
          super().__init__()
          self._name = name

      @abstractmethod
      def __call__(self, state):
          """Select an action based on a state."""
          raise NotImplementedError


  class RLPolicy(AbsPolicy):     
      ...

      def record(self, key: str, state, action, reward, next_state, terminal: bool):
          pass

      def get_rollout_info(self):
          pass

      def get_batch_loss(self, batch: dict, explicit_grad: bool = False):
          pass

      def update(self, loss_info_list: List[dict]):
          pass

      def learn(self, batch: dict):
          pass

      def improve(self):
          pass


.. _policy-manager:

Policy Manager
--------------

A policy manager controls policy update and provides the latest policy states for roll-outs. In synchrounous learning,
the policy manager controls the policy update phase of a learning cycle. In asynchrounous learning, the policy manager
is present as a server process. The types of policy manager include:

* ``SimplePolicyManager``, where all policies instances reside within the manager and are updated sequentially;
* ``MultiProcessPolicyManager``, where each policy is placed in a dedicated process that runs event loops to receive
  roll-out information for update; this approach takes advantage of the parallelism from Python's multi-processing, so
  the policies can be updated in parallel, but comes with inter-process communication overhead;
* ``DistributedPolicyManager``, where policies are distributed among a set of remote compute nodes that run event loops
  to reeeive roll-out information for update. This approach allows the policies to be updated in parallel and may be
  necessary when the combined size of the policies is too big to fit in a single node. 

Moreover, in ``data-parallel`` mode, each policy manager has an additional worker(``grad_worker``)
allocator, which provides a policy-to-worker mapping. The worker allocator performs auto-balance
during training, by dynamically adjusting worker number for policies according to the
experience/agent/policy number.

.. image:: ../images/rl/policy_manager.svg
   :target: ../images/rl/policy_manager.svg
   :alt: PolicyManager

The ``DistributedPolicyManager`` runs a set of ``policy_host`` and a ``TrainerAllocator``.
``policy_host`` is a process/VM/node that hosts the update of a policy. The ``TrainerAllocator``
dynamically adjusts worker node numbers for policies according to the experience/agent/policy
number. Each ``policy_host`` independently updates its own policies for policy-level parallelism. 

During training, the ``PolicyManager`` receives training data collected by the ``RolloutManager``,
then send them to corresponding ``policy_host``. Each ``policy_host`` will send gradient tasks consist
of policy state and experience batch, to several stateless ``grad_worker`` for gradient computation.
The ``grad_worker`` is stateless, and computes gradients using the policy state and data
batch provided in a task.
Then ``policy_host`` aggregates the gradients from ``grad_worker`` s, and performs gradient descent
on its parameters.

Core Model
----------

In the deep reinforcement learning (DRL) world, a policy usually includes one or more neural-network com-based models,
which may be used to compute action preferences or estimate state / action values. The ``AbsCoreModel`` represents a
collection of network components with embedded optimizers and exposes unified interfaces to decouple model inference
and optimization from the algorithmic aspects of the policy that uses them. For example, the actor-critic algorithm
does not need to concern itself with how the action probabilities and state values are computed. Subclasses of
``AbsCoreModel`` provided for use with specific RL algorithms include ``DiscreteQNet`` for DQN, ``DiscretePolicyNet``
for Policy Gradient, ``DiscreteACNet`` for Actor-Critic and ``ContinuousACNet`` for DDPG.

The code snippet below shows how to create a model for the actor-critic algorithm with a shared bottom stack:

.. code-block:: python

  shared_net_conf = {...}
  actor_net_conf = {...}
  critic_net_conf = {...}
  shared_optim_conf = {torch.optim.SGD, {"lr": 0.0001}}
  actor_optim_conf = (torch.optim.Adam, {"lr": 0.001})
  critic_optim_conf = (torch.optim.RMSprop, {"lr": 0.001})

  class MyACNet(DiscreteACNet):
      def __init__(self):
          super().__init__()
          self.shared = FullyConnected(**shared_net_conf)
          self.actor = FullyConnected(**actor_net_conf)
          self.critic = FullyConnected(**critic_net_conf)
          self.shared_optim = shared_optim_conf[0](self.shared.parameters(), **shared_optim_conf[1])
          self.actor_optim = actor_optim_conf[0](self.actor.parameters(), **actor_optim_conf[1])
          self.critic_optim = critic_optim_conf[0](self.critic.parameters(), **critic_optim_conf[1])

      def forward(self, states, actor: bool = True, critic: bool = True):
          representation = self.shared(states)
          return (self.actor(representation) if actor else None), (self.critic(representation) if critic else None)

      def step(self, loss):
          self.shared_optim.zero_grad()
          self.actor_optim.zero_grad()
          self.critic_optim.zero_grad()
          loss.backward()
          self.shared_optim.step()
          self.actor_optim.step()
          self.critic_optim.step()

To generate stochastic actions given a batch of states, call ``get_action`` on the model instance: 

.. code-block:: python

  action, log_p, values = ac_model.get_action(state)

To performing a single gradient step on the model, pass the loss to the ``step`` function: 

.. code-block:: python

  ac_model.step(critic_loss + actor_loss)
